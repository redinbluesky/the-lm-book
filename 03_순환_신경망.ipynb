{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffd5c015",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/the-lm-book/blob/main/03_순환_신경망.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9796d8b8",
   "metadata": {},
   "source": [
    "# 목차\n",
    "* [Chapter 3_1 엘만 RNN](#chapter3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e997bd8",
   "metadata": {},
   "source": [
    "## Chapter 3_1 엘만 RNN <a class=\"anchor\" id=\"chapter3_1\"></a>\n",
    "1. 순환 신경망(Recurrent Neural Network, RNN)은 순차 데이터(sequential data)를 위해 고안된 신경망이다.\n",
    "    - 유닛 사이에 있는 연결에 루프가 포함되어 있어 시퀸스에서 한 단계의 정보가 다음 단계로 전달 될 수 있다.\n",
    "    - 시계열 분석, 자연어 처리와 같은 분야에서 주로 사용된다.\n",
    "\n",
    "2. 하나의 유닛을 가진 신경망과 입력 문서 \"Learning from text is cool\"이 있다고 가정하자.\n",
    "    - 대소문자와 구두점을 무시하고 이 문서를 다음과 같은 행렬로 표현할 수 있다.\n",
    "    <pre>\n",
    "         단어     임베딩 벡터\n",
    "         learning [0.1, 0.2, 0.6]\n",
    "         from     [0.2, 0.1, 0.4]\n",
    "         text     [0.1, 0.3, 0.5]\n",
    "         is       [0.0, 0.7, 0.1]\n",
    "         cool     [0.5, 0.2, 0.7]\n",
    "         PAD      [0.0, 0.0, 0.0] (패딩 토큰)\n",
    "    </pre>\n",
    "    - 각 행은 신경망 훈련 과정에서 학습된 단어 임베딩 벡터이다.\n",
    "    - 단어의 순서는 보존되며, 행렬 차원은 (시퀸스 길이, 임베딩차원) =(6, 3)이다.\n",
    "       - 시퀸스 길이는 문서에 있는 최대 단어 수\n",
    "    - 시퀸스 길이보다 짧은 문서는 패딩 토큰(PAD)으로 채운다.\n",
    "       - 패딩 토큰은 더미 임베딩으로 일반적으로 0 벡터를 사용한다.\n",
    "    - 수학적으로 행렬을 표현하면 아래와 같다.\n",
    "    <pre>\n",
    "    X = [ [0.1, 0.2, 0.6],\n",
    "          [0.2, 0.1, 0.4],\n",
    "          [0.1, 0.3, 0.5],\n",
    "          [0.0, 0.7, 0.1],\n",
    "          [0.5, 0.2, 0.7],\n",
    "          [0.0, 0.0, 0.0] ]\n",
    "    </pre>\n",
    "    - 5개의 3차원 임베딩 벡터 x₁, x₂, x₃, x₄, x₅가 문서에 있는 단어를 나타낸다.\n",
    "        - learning / x₁ = [0.1, 0.2, 0.6]ᵀ, from / x₂ = [0.2, 0.1, 0.4]ᵀ, text / x₃ = [0.1, 0.3, 0.5]ᵀ, \n",
    "        - is / x₄ = [0.0, 0.7, 0.1]ᵀ, cool / x₅ = [0.5, 0.2, 0.7]ᵀ\n",
    "\n",
    "3. 엘만 RNN(Elman RNN)은 순환 신경망의 한 종류이다.\n",
    "    - 아래의 그림과 같이 임베딩 벡터의 시퀸스를 한번에 하나씩 처리한다.\n",
    "    - 각 타임 스텝 t에서 현재 입력 임베딩 벡터 xₜ와 이전 은닉 상태 hₜ₋₁를 훈련 가능한 가중치 Wₕ, Uₕ와 곱해진 후 편향 벡터 b와 더해져 은닉층 hₜ이 계산된다.\n",
    "    - 스칼라를 출력하는 MLP 유닛과 달리 RNN 유닛은 벡터를 출력하고 하나의 층처럼 동작한다.\n",
    "    - 초기 은닉상태 h₀는 일반적으로 0 벡터로 설정된다.\n",
    "\n",
    "        ![Elman RNN](image/03-01-엘만_RNN.png)\n",
    "\n",
    "    - 은닉 상태는 시퀸스에 있는 이전 단계의 정보를 기억하는 메모리 벡터이다.\n",
    "    - 현재 입력과 지난 은닉 상태를 사용해 매 스탭마다 업데이트 되며, 신경망이 앞선 단어의 문맥을 사용해 다음 단어를 예측할 수 있도록 돕는다.\n",
    "    - 심층 신경망을 만들려면 두 번째 RNN 층을 추가한다. 첫 번째 층의 출력이 두 번째 층의 입력이 된다.\n",
    "\n",
    "        ![Stacked Elman RNN](image/03-01-스택_엘만_RNN.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
