{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3461b873",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/the-lm-book/blob/main/02_언어_모델링_기초.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffb53f",
   "metadata": {},
   "source": [
    "# 목차\n",
    "* [Chapter 2_1 BoW](#chapter2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa90e9",
   "metadata": {},
   "source": [
    "## Chapter 2_1 BoW <a class=\"anchor\" id=\"chapter2_1\"></a>\n",
    "1. 문서 집합이 있고 각 문서의 주요 토픽을 예측하고 싶다고 가정한다.\n",
    "   - 토픽이 사전에 정의가 되어 있다면 이 작업은 분류 작업이 된다.\n",
    "      - 두 개의 토픽이 있다면 이진 분류, 세 개 이상의 토픽이 있다면 다중 분류가 된다.\n",
    "   \n",
    "2. 다중 분류에서 데이터 셋은 다음과 같이 표시할 수 있다.\n",
    "   - {(xᵢ, yᵢ)}ᵢ₌₁ⁿ 쌍으로 구성된다.\n",
    "      - xᵢ: 문서를 나타태는 텍스트\n",
    "      - n: 샘플의 개수\n",
    "      - yᵢ: 문서의 토픽을 나타내는 정수\n",
    "         - yᵢ ∈ {1, 2, ..., C} C는 가능한 토픽의 개수\n",
    "         - 예를 들어 1은 \"music\", 2는 \"sports\", 3은 \"politics\"를 나타낼 수 있다.\n",
    "\n",
    "3. 기계는 사람처럼 텍스트를 처리하지 못한다.\n",
    "   - 텍스트를 숫자 벡터로 변환해야 한다.\n",
    "   \n",
    "4. Bag-of-Words(BoW) 모델은 텍스트를 숫자 벡터로 변환하는 간단한 방법이다.\n",
    "   - 10개의 문서를 예를들어 작동 방식을 설명한다.\n",
    "     - 문서 텍스트\n",
    "         1. Movies are fun for everyone.\n",
    "         2. Watching movies is great fun.\n",
    "         3. Enjoy a great movie today.\n",
    "         4. Research is interesting and important.\n",
    "         5. Learning math is very important.\n",
    "         6. Science discovery is interesting.\n",
    "         7. Rock is great to listen to.\n",
    "         8. Listen to music for fun.\n",
    "         9. Music is fun for everyone.\n",
    "         10. Listen to folk music! \n",
    "\n",
    "5. 머신러닝에서 사용되는 텍스트 문서 집합을 말뭉치(corpus)라고 하며 BoW 방법을 말뭉치에 적용하려면 두 단계를 거친다.\n",
    "   - 어휘사전을 만든다\n",
    "      - 말뭉치에 등장하는 고유 단어를 나열해 어휘사전(vocabulary)을 만든다.\n",
    "   - 문서를 벡터로 변환한다.\n",
    "      - 각각의 문서를 특성 벡터로 변환한다.\n",
    "      - 이 벡터의 각 차원은 어휘사전에 있는 한 단어를 나타낸다.\n",
    "      - 벡터에는 단어 있음/없음, 문서에서의 등장빈도가 입력된다.\n",
    "\n",
    "6. 10개의 문서로 구성된 말뭉치를 사용해 모든 고유한 단어를 알파펫 순서로 나열해 어휘사전을 만든다.\n",
    "   - 구두점을 삭제하고, 단어를 소문자로 변경하고, 중복 단어는 삭제한다.\n",
    "   - vocabulary = [\"a\", \"and\", \"are\", \"discovery\", \"enjoy\", \"everyone\", \"folk\", \"for\", \"fun\", \"great\", \"important\", \"interesting\", \"is\", \"listen\", \"learning\", \"math\", \"movie\", \"movies\", \"music\", \"of\", \"rock\", \"science\", \"to\", \"very\", \"watching\"]\n",
    "\n",
    "7. 문서를 더 나눌 수 없는 작은 부분으로 분활하는 것을 토큰화(tokenization)이라고한다.\n",
    "   - 분할된 각 부분을 토큰(token)이라고 한다.\n",
    "\n",
    "8. 때때로 단어를 더 작은 단위인 부분단어(subword)로 분할하여 어휘사전을 적절한 크기로 유지하는 게 유용할 때가 있습니다.\n",
    "   - 예를 들어 \"unhappiness\"라는 단어를 \"un\", \"happi\", \"ness\"라는 세 개의 부분단어로 분할할 수 있다.\n",
    "   - 부분단어 토큰화는 특히 희귀 단어와 신조어에 유용하다.\n",
    "   - 날리 사용되는 부분단어 토큰화 알고리즘으로는 Byte Pair Encoding(BPE)과 WordPiece가 있다.\n",
    "   - do, does, doing, did와 같이 모든 영어 단어의 표면형태(surface form)는 수백만 개가 될 수 있고, 더 복잡한 형태를 가진 언어는 그 개수가 훨씬 더 많다.\n",
    "   - 모든 단어의 표면형태를 어휘사전에 포함시키는 것은 비현실적이다.\n",
    "   - 부분단어 토큰화를 사용하면 어휘사전의 크기를 줄이고, 모델이 훈련 데이터에 나타나지 않는 단어를 처리할 수 있게 한다\n",
    "\n",
    "9. 단어는 토큰의 한 형태로 문서에서 더 나눌 수 없는 단위를 의미하며, 토큰과 단어는 혼용되어 사용되는 경우가 많다.\n",
    "   - BoW는 단어와 부분단어 모두를 다룰 수 있지만, 원래 단어를 위해 설계되었다.\n",
    "\n",
    "10. 특성 벡터로 문서 단어 행렬(Document-Term Matrix, DTM)을 만들 수 있다.\n",
    "   - 행은 문서, 열은 어휘사전의 단어(토큰)를 나타낸다.\n",
    "   - 각 셀에는 해당 단어가 문서에 등장한 횟수가 들어간다.\n",
    "   - 아래의 이미지에서 DTM의 각 행은 문서를 나타내고, 각 열은 어휘사전의 단어를 나타낸다.\n",
    "   - 만약 단어가 문서에 등장하지 않으면 해당 셀에는 0이 들어가고, 등장하면 등장 횟수가 들어간다.\n",
    "\n",
    "      ![Document-Term Matrix](image/02-01-doc-term-matrix.png)\n",
    "\n",
    "11. 문서 2 \"Watching movies is great fun.\"의 특성 벡터는 다음과 같다.\n",
    "   - [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
    "   - 이 벡터에서 세 번째 차원(단어 \"are\")은 0이다. 이는 \"are\"가 문서에 등장하지 않음을 의미한다.\n",
    "   - 아홉 번째 차원(단어 \"fun\")은 1이다. 이는 \"fun\"이 문서에 한 번 등장했음을 의미한다.\n",
    "\n",
    "12. 자연어에서 단어의 빈도는 지프의 법칙(Zipf's Law)을 따른다.\n",
    "   - 단어를 빈도순으로 나열했들 때 단어의 빈도가 순서에 반비례한다는 것이다.\n",
    "   - 즉, 가장 빈도가 높은 단어는 두 번째로 빈도가 높은 단어의 두 배, 세 번째로 빈도가 높은 단어의 세 배가 된다.\n",
    "   - 이러한 현상은 대부분의 언어에서 관찰된다.\n",
    "   - 예를 들어 영어에서 \"the\", \"is\", \"in\"과 같은 단어는 매우 자주 등장하는 반면, \"quixotic\", \"serendipity\"와 같은 단어는 매우 드물게 등장한다.\n",
    "   - 지프의 법칙은 단어 빈도가 매우 불균등하게 분포되어 있음을 의미한다.\n",
    "   - 이는 BoW 표현에서 많은 단어가 대부분의 문서에서 등장하지 않음을 의미한다.\n",
    "   - 결과적으로 DTM은 매우 희소(sparse)해진다.\n",
    "   - 희소 행렬은 대부분의 요소가 0인 행렬을 의미한다.\n",
    "   - 희소 행렬은 메모리 효율성과 계산 효율성 측면에서 도전 과제를 제기할 수 있다.\n",
    "\n",
    "13. 이런 특성 벡터를 사용해 문서의 토픽을 예측하도록 신경망을 훈련시킬 수 있다.\n",
    "   - 문서에 레비블을 할당하는 레이블링을 수행한다.\n",
    "      - 레이블링은 수작업이나 알로리즘을 통해 수행할 수 있다.\n",
    "      - 알고리즘을 사용하는 경우 정확성을 확보하기 위해 사람이 검증해야 하는 경우가 많다.\n",
    "      - 문서를 직접 잀고 세 개의 토픽 중 가장 적합한 것을 골라 레이블을 할당한다. \n",
    "   - 문서 텍스트                                       클래스ID    클래스 이름\n",
    "         1. Movies are fun for everyone.               1          Cinema\n",
    "         2. Watching movies is great fun.              1          Cinema\n",
    "         3. Enjoy a great movie today.                 1          Cinema\n",
    "         4. Research is interesting and important.     2          Science\n",
    "         5. Learning math is very important.           2          Science\n",
    "         6. Science discovery is interesting.          2          Science\n",
    "         7. Rock is great to listen to.                3          Music\n",
    "         8. Listen to music for fun.                   3          Music\n",
    "         9. Music is fun for everyone.                 3          Music\n",
    "         10. Listen to folk music!                     3          Music\n",
    "\n",
    "14. 고급 채팅 언어 모델(chat language model, ChatLM)은 전문가 모델의 패널(panel)을 활용해 자동화된 문서 레이블링을 매우 정확하게 수행할 수 있다.\n",
    "   - 세 개의 LLM을 사용하는 경우 두 개 이상의 모델이 문서에 동일하게 할당한 레이블을 채택한다.\n",
    "   - 세 개의 LLM이 뫼두 다른 레이블을 예측하면 사람이 결정하거나 네 번째 모델을 사용해 교착 상태를 해결할 수 있다.\n",
    "   - 많은 비즈니스 문제에서 LLM이 빠르고 더 안정적인 레이블링을 제공하므로 수동 레이블링은 점점 사라지고 있다.\n",
    "\n",
    "15. 이진 분류기는 일반적으로 시그모이드 활성화 함수와 이진 크로스 엔트로피 손실을 사용한다.\n",
    "   - 세 개 이상의 클래스를 다루는 작업에서는 일반저긍로 소프트맥스 활성화 함수와 크로스 엔트로피 손실을 사용한다.\n",
    "   - 소프트맥스 함수는 다음과 같이 정의된다.\n",
    "      - softmax(zᵢ, k) = eᶻₖ / Σⱼ₌₁ᴰeᶻⱼ\n",
    "         - zᵢ: D차원의 로짓 벡터, 로짓은 활성화 함수를 적용하기 전의 신경망 출력값\n",
    "         - k: 소프트맥스가 계산되는 인덱스\n",
    "         - e: 자연상수(오일러수 약 2.718)\n",
    "         - Σⱼ: 모든 클래스 j에 대한 합계\n",
    "\n",
    "16. 아래 그림은 한 신경망의 출력 층 o를 나타낸다.\n",
    "   - 로짓 zₒ는 밝은 녹샛 상자 안의 값으로 활성화 함수가 적용되기 전의 신경망 출력값이다.\n",
    "   - Zₒ= [zₒ₁, zₒ₂, zₒ₃]ᵀ\n",
    "   \n",
    "      ![Softmax Function](image/02-01-softmax-function.png)\n",
    "\n",
    "   - 예를 들어 유닛 o, 2에 대한 소프트맥스는 다음과 같이 계산된다.\n",
    "      - softmax(zₒ₂, 2) = eᶻₒ₂ / (eᶻₒ₁ + eᶻₒ₂ + eᶻₒ₃)  \n",
    "   - 소프트맥스 함수는 벡터를 이산 확률 분포(discrete probability distribution)로 변환한다.\n",
    "      - 유한집합의 값에 확률을 할당하여 그 합이 1이되되록 한다.\n",
    "         - 유한집합이란 셀수 있는 개수의 원소가 포함된 집합이다.\n",
    "         - 클래스 1, 클래스 2, 클래스 3의 세 개의 클래스가 있는 경우 유한집합은 {1, 2, 3}이 된다.\n",
    "         - 소프트맥스 함수는 각 클래스를 확률로 매핑하며, 모든 클래스의 확률 합은 1이 된다.\n",
    "      - Σⱼ₌₁ᴰsoftmax(zᵢ, j) = 1\n",
    "\n",
    "17. \"cinema\", \"music\", \"science\" 세 개의 로짓이 z = [2.0, 1.0, 0.5]라고 가정한다.\n",
    "   - 각 로직에 대한 오일러 지수를 계산한다.\n",
    "      - ez¹ = e².⁰ ≈ 7.389\n",
    "      - ez² = e¹.⁰ ≈ 2.718\n",
    "      - ez³ = e⁰.⁵ ≈ 1.649 \n",
    "   - 모든 값을 더한다.\n",
    "      - Σ = 7.389 + 2.718 + 1.649 ≈ 11.756\n",
    "   - 소프트맥스 함수 공식을 사용해 확률을 계산한다.\n",
    "      - softmax(z, 1) = ez¹ / Σ ≈ 7.389 / 11.756 ≈ 0.628\n",
    "      - softmax(z, 2) = ez² / Σ ≈ 2.718 / 11.756 ≈ 0.231\n",
    "      - softmax(z, 3) = ez³ / Σ ≈ 1.649 / 11.756 ≈ 0.140\n",
    "   - 따라서 클래스 \"cinema\"의 확률은 약 62.8%, \"music\"의 확률은 약 23.1%, \"science\"의 확률은 약 14.0%이다.\n",
    "   - 세 확률의 합은 1이 된다.\n",
    "      - 0.628 + 0.231 + 0.140 ≈ 0.999 (반올림으로 인해 약간의 오차가 발생할 수 있음)\n",
    "\n",
    "18. 신경망의 소프트맥스 출력은 모두 더하면 1이고 클래스 가능도와 닮았지만, 진정한 통계적 확률보다는 '확률 점수'로 이해하는 것이 낫다.\n",
    "   - 신경망이 출력하는 값은 모델이 각 클래스에 대해 얼마나 확신하는지를 나타내는 점수이다.\n",
    "   - 이 점수는 실제 확률과 다를 수 있다.\n",
    "   - 예를 들어, 모델이 클래스 A에 대해 0.9의 확률을 출력했더라도, 실제로는 클래스 A가 정답일 확률이 0.7일 수 있다.\n",
    "   - 따라서 소프트맥스 출력은 모델의 신뢰도를 나타내는 지표로 사용해야 한다.\n",
    "\n",
    "19. 크로스 엔트로피 손실은 예측 확률이 정답 분포와 얼마나 잘 맞는지를 측정한다.\n",
    "   - 정답 분포는 일반적으로 원삿 벡터(one-hot vector)로 표현된다.\n",
    "      - 원삿 벡터는 정답 클래스에 1을 할당하고 나머지 클래스에는 0을 할당하는 벡터이다.\n",
    "   - 클래스 1 = [1, 0, 0], 클래스 2 = [0, 1, 0], 클래스 3 = [0, 0, 1]로 표현된다.\n",
    "   - 하나의 크로스 엔트로피 손실은 다음과 같이 계산한다.\n",
    "      - loss(ỹ, y) = - Σⱼ₌₁ᴰ yⱼ log(ỹⱼ)\n",
    "         - ỹ: 신경망의 소프트맥스 출력 확률 벡터\n",
    "         - y: 정답 원삿 벡터\n",
    "         - D: 클래스의 개수\n",
    "         - log: 자연로그\n",
    "         - yⱼ: 정답 원삿 벡터의 j번째 요소\n",
    "         - ỹⱼ: 신경망의 소프트맥스 출력 확률의 j번째 요소\n",
    "         - y가 원핫 인코딩되어 있으므로 정답 클래스에 해당하는 원소만 덧셈 계산에 포함된다.\n",
    "            - 하나의 원소만 남겨 이 덧셈을 단순화할 수 있다.\n",
    "      - 정답 클래스가 c라고 가정하면 yc = 1이고 k=!c인 모든 경우에 yk = 0이 된다.\n",
    "         - 따라서 크로스 엔트로피 손실은 다음과 같이 단순화된다.\n",
    "            - loss(ỹ, y) = - log(ỹc)\n",
    "         - 간소화된 이 식은 손실이 정답 클래스에 할당된 확률의 음의 로그에 해당하는 것을 보여준다.\n",
    "      - N개의 샘플이 있는 경우 편균 손실은 다음과 같이 계산된다.\n",
    "         - Loss = (1/N) Σᵢ₌₁ᴺ log(ỹcᵢ)\n",
    "            - cᵢ: i번재 샘플에 대한 정답 클래스 인덱스\n",
    "\n",
    "20. 출력 층에 소프트맥스 함수를 사용할 때 크로스 엔트로피 손실은 신경망이 정답 클래스에는 높은 확률을 할당하고, 다른 클래스에는 낮은 확률을 할당하도록 유도한다.\n",
    "   - \"cinema\", \"music\", \"science\" 세 개의 클래스를 가진 문서 분류 예제의 경우 신경망의 세 개의 로짓을 생성한다.\n",
    "   - 이 로짓이 소프트맥스 함수를 통과하여 각 클래스에 대한 확률로 변환된다.\n",
    "   - 이 점수와 원핫 인코딩된 정답 레이블로 크로스 엔트로피 손실이 계산된다.\n",
    "   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
