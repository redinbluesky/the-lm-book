{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3461b873",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/the-lm-book/blob/main/02_언어_모델링_기초.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffb53f",
   "metadata": {},
   "source": [
    "# 목차\n",
    "* [Chapter 2_1 BoW](#chapter2_1)\n",
    "* [Chapter 2_2 단어 임베딩](#chapter2_2)\n",
    "* [Chapter 2_3 바이트 페어 인코딩](#chapter2_3)\n",
    "* [Chapter 2_4 언어모델](#chapter2_4)\n",
    "* [Chapter 2_5 카운트 기반 언어 모델](#chapter2_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa90e9",
   "metadata": {},
   "source": [
    "## Chapter 2_1 BoW <a class=\"anchor\" id=\"chapter2_1\"></a>\n",
    "1. 문서 집합이 있고 각 문서의 주요 토픽을 예측하고 싶다고 가정한다.\n",
    "   - 토픽이 사전에 정의가 되어 있다면 이 작업은 분류 작업이 된다.\n",
    "      - 두 개의 토픽이 있다면 이진 분류, 세 개 이상의 토픽이 있다면 다중 분류가 된다.\n",
    "   \n",
    "2. 다중 분류에서 데이터 셋은 다음과 같이 표시할 수 있다.\n",
    "   - {(xᵢ, yᵢ)}ᵢ₌₁ⁿ 쌍으로 구성된다.\n",
    "      - xᵢ: 문서를 나타태는 텍스트\n",
    "      - n: 샘플의 개수\n",
    "      - yᵢ: 문서의 토픽을 나타내는 정수\n",
    "         - yᵢ ∈ {1, 2, ..., C} C는 가능한 토픽의 개수\n",
    "         - 예를 들어 1은 \"music\", 2는 \"sports\", 3은 \"politics\"를 나타낼 수 있다.\n",
    "\n",
    "3. 기계는 사람처럼 텍스트를 처리하지 못한다.\n",
    "   - 텍스트를 숫자 벡터로 변환해야 한다.\n",
    "   \n",
    "4. Bag-of-Words(BoW) 모델은 텍스트를 숫자 벡터로 변환하는 간단한 방법이다.\n",
    "   - 10개의 문서를 예를들어 작동 방식을 설명한다.\n",
    "     - 문서 텍스트\n",
    "         1. Movies are fun for everyone.\n",
    "         2. Watching movies is great fun.\n",
    "         3. Enjoy a great movie today.\n",
    "         4. Research is interesting and important.\n",
    "         5. Learning math is very important.\n",
    "         6. Science discovery is interesting.\n",
    "         7. Rock is great to listen to.\n",
    "         8. Listen to music for fun.\n",
    "         9. Music is fun for everyone.\n",
    "         10. Listen to folk music! \n",
    "\n",
    "5. 머신러닝에서 사용되는 텍스트 문서 집합을 말뭉치(corpus)라고 하며 BoW 방법을 말뭉치에 적용하려면 두 단계를 거친다.\n",
    "   - 어휘사전을 만든다\n",
    "      - 말뭉치에 등장하는 고유 단어를 나열해 어휘사전(vocabulary)을 만든다.\n",
    "   - 문서를 벡터로 변환한다.\n",
    "      - 각각의 문서를 특성 벡터로 변환한다.\n",
    "      - 이 벡터의 각 차원은 어휘사전에 있는 한 단어를 나타낸다.\n",
    "      - 벡터에는 단어 있음/없음, 문서에서의 등장빈도가 입력된다.\n",
    "\n",
    "6. 10개의 문서로 구성된 말뭉치를 사용해 모든 고유한 단어를 알파펫 순서로 나열해 어휘사전을 만든다.\n",
    "   - 구두점을 삭제하고, 단어를 소문자로 변경하고, 중복 단어는 삭제한다.\n",
    "   - vocabulary = [\"a\", \"and\", \"are\", \"discovery\", \"enjoy\", \"everyone\", \"folk\", \"for\", \"fun\", \"great\", \"important\", \"interesting\", \"is\", \"listen\", \"learning\", \"math\", \"movie\", \"movies\", \"music\", \"of\", \"rock\", \"science\", \"to\", \"very\", \"watching\"]\n",
    "\n",
    "7. 문서를 더 나눌 수 없는 작은 부분으로 분활하는 것을 토큰화(tokenization)이라고한다.\n",
    "   - 분할된 각 부분을 토큰(token)이라고 한다.\n",
    "\n",
    "8. 때때로 단어를 더 작은 단위인 부분단어(subword)로 분할하여 어휘사전을 적절한 크기로 유지하는 게 유용할 때가 있습니다.\n",
    "   - 예를 들어 \"unhappiness\"라는 단어를 \"un\", \"happi\", \"ness\"라는 세 개의 부분단어로 분할할 수 있다.\n",
    "   - 부분단어 토큰화는 특히 희귀 단어와 신조어에 유용하다.\n",
    "   - 날리 사용되는 부분단어 토큰화 알고리즘으로는 Byte Pair Encoding(BPE)과 WordPiece가 있다.\n",
    "   - do, does, doing, did와 같이 모든 영어 단어의 표면형태(surface form)는 수백만 개가 될 수 있고, 더 복잡한 형태를 가진 언어는 그 개수가 훨씬 더 많다.\n",
    "   - 모든 단어의 표면형태를 어휘사전에 포함시키는 것은 비현실적이다.\n",
    "   - 부분단어 토큰화를 사용하면 어휘사전의 크기를 줄이고, 모델이 훈련 데이터에 나타나지 않는 단어를 처리할 수 있게 한다\n",
    "\n",
    "9. 단어는 토큰의 한 형태로 문서에서 더 나눌 수 없는 단위를 의미하며, 토큰과 단어는 혼용되어 사용되는 경우가 많다.\n",
    "   - BoW는 단어와 부분단어 모두를 다룰 수 있지만, 원래 단어를 위해 설계되었다.\n",
    "\n",
    "10. 특성 벡터로 문서 단어 행렬(Document-Term Matrix, DTM)을 만들 수 있다.\n",
    "   - 행은 문서, 열은 어휘사전의 단어(토큰)를 나타낸다.\n",
    "   - 각 셀에는 해당 단어가 문서에 등장한 횟수가 들어간다.\n",
    "   - 아래의 이미지에서 DTM의 각 행은 문서를 나타내고, 각 열은 어휘사전의 단어를 나타낸다.\n",
    "   - 만약 단어가 문서에 등장하지 않으면 해당 셀에는 0이 들어가고, 등장하면 등장 횟수가 들어간다.\n",
    "\n",
    "      ![Document-Term Matrix](image/02-01-doc-term-matrix.png)\n",
    "\n",
    "11. 문서 2 \"Watching movies is great fun.\"의 특성 벡터는 다음과 같다.\n",
    "   - [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
    "   - 이 벡터에서 세 번째 차원(단어 \"are\")은 0이다. 이는 \"are\"가 문서에 등장하지 않음을 의미한다.\n",
    "   - 아홉 번째 차원(단어 \"fun\")은 1이다. 이는 \"fun\"이 문서에 한 번 등장했음을 의미한다.\n",
    "\n",
    "12. 자연어에서 단어의 빈도는 지프의 법칙(Zipf's Law)을 따른다.\n",
    "   - 단어를 빈도순으로 나열했들 때 단어의 빈도가 순서에 반비례한다는 것이다.\n",
    "   - 즉, 가장 빈도가 높은 단어는 두 번째로 빈도가 높은 단어의 두 배, 세 번째로 빈도가 높은 단어의 세 배가 된다.\n",
    "   - 이러한 현상은 대부분의 언어에서 관찰된다.\n",
    "   - 예를 들어 영어에서 \"the\", \"is\", \"in\"과 같은 단어는 매우 자주 등장하는 반면, \"quixotic\", \"serendipity\"와 같은 단어는 매우 드물게 등장한다.\n",
    "   - 지프의 법칙은 단어 빈도가 매우 불균등하게 분포되어 있음을 의미한다.\n",
    "   - 이는 BoW 표현에서 많은 단어가 대부분의 문서에서 등장하지 않음을 의미한다.\n",
    "   - 결과적으로 DTM은 매우 희소(sparse)해진다.\n",
    "   - 희소 행렬은 대부분의 요소가 0인 행렬을 의미한다.\n",
    "   - 희소 행렬은 메모리 효율성과 계산 효율성 측면에서 도전 과제를 제기할 수 있다.\n",
    "\n",
    "13. 이런 특성 벡터를 사용해 문서의 토픽을 예측하도록 신경망을 훈련시킬 수 있다.\n",
    "   - 문서에 레이블을 할당하는 레이블링을 수행한다.\n",
    "      - 레이블링은 수작업이나 알로리즘을 통해 수행할 수 있다.\n",
    "      - 알고리즘을 사용하는 경우 정확성을 확보하기 위해 사람이 검증해야 하는 경우가 많다.\n",
    "      - 문서를 직접 잀고 세 개의 토픽 중 가장 적합한 것을 골라 레이블을 할당한다. \n",
    "   - 이미지로 표현하면 아래와 같다.\n",
    "   \n",
    "      ![Document Labeling](image/02-01-document-labeling.png)\n",
    "    \n",
    "\n",
    "14. 고급 채팅 언어 모델(chat language model, ChatLM)은 전문가 모델의 패널(panel)을 활용해 자동화된 문서 레이블링을 매우 정확하게 수행할 수 있다.\n",
    "   - 세 개의 LLM을 사용하는 경우 두 개 이상의 모델이 문서에 동일하게 할당한 레이블을 채택한다.\n",
    "   - 세 개의 LLM이 모두 다른 레이블을 예측하면 사람이 결정하거나 네 번째 모델을 사용해 교착 상태를 해결할 수 있다.\n",
    "   - 많은 비즈니스 문제에서 LLM이 빠르고 더 안정적인 레이블링을 제공하므로 수동 레이블링은 점점 사라지고 있다.\n",
    "\n",
    "15. 이진 분류기는 일반적으로 시그모이드 활성화 함수와 이진 크로스 엔트로피 손실을 사용한다.\n",
    "   - 세 개 이상의 클래스를 다루는 작업에서는 일반적으로 소프트맥스 활성화 함수와 크로스 엔트로피 손실을 사용한다.\n",
    "   - 소프트맥스 함수는 다음과 같이 정의된다.\n",
    "      - softmax(zᵢ, k) = eᶻₖ / Σⱼ₌₁ᴰeᶻⱼ\n",
    "         - zᵢ: D차원의 로짓 벡터, 로짓은 활성화 함수를 적용하기 전의 신경망 출력값\n",
    "         - k: 소프트맥스가 계산되는 인덱스\n",
    "         - e: 자연상수(오일러수 약 2.718)\n",
    "         - Σⱼ: 모든 클래스 j에 대한 합계\n",
    "\n",
    "16. 아래 그림은 한 신경망의 출력 층 o를 나타낸다.\n",
    "   - 로짓 zₒ는 밝은 녹샛 상자 안의 값으로 활성화 함수가 적용되기 전의 신경망 출력값이다.\n",
    "   - Zₒ= [zₒ₁, zₒ₂, zₒ₃]ᵀ\n",
    "   \n",
    "      ![Softmax Function](image/02-01-softmax-function.png)\n",
    "\n",
    "   - 예를 들어 유닛 o, 2에 대한 소프트맥스는 다음과 같이 계산된다.\n",
    "      - softmax(zₒ₂, 2) = eᶻₒ₂ / (eᶻₒ₁ + eᶻₒ₂ + eᶻₒ₃)  \n",
    "   - 소프트맥스 함수는 벡터를 이산 확률 분포(discrete probability distribution)로 변환한다.\n",
    "      - 유한집합의 값에 확률을 할당하여 그 합이 1이되되록 한다.\n",
    "         - 유한집합이란 셀수 있는 개수의 원소가 포함된 집합이다.\n",
    "         - 클래스 1, 클래스 2, 클래스 3의 세 개의 클래스가 있는 경우 유한집합은 {1, 2, 3}이 된다.\n",
    "         - 소프트맥스 함수는 각 클래스를 확률로 매핑하며, 모든 클래스의 확률 합은 1이 된다.\n",
    "      - Σⱼ₌₁ᴰsoftmax(zᵢ, j) = 1\n",
    "\n",
    "17. \"cinema\", \"music\", \"science\" 세 개의 로짓이 z = [2.0, 1.0, 0.5]라고 가정한다.\n",
    "   - 각 로직에 대한 오일러 지수를 계산한다.\n",
    "      - ez¹ = e².⁰ ≈ 7.389\n",
    "      - ez² = e¹.⁰ ≈ 2.718\n",
    "      - ez³ = e⁰.⁵ ≈ 1.649 \n",
    "   - 모든 값을 더한다.\n",
    "      - Σ = 7.389 + 2.718 + 1.649 ≈ 11.756\n",
    "   - 소프트맥스 함수 공식을 사용해 확률을 계산한다.\n",
    "      - softmax(z, 1) = ez¹ / Σ ≈ 7.389 / 11.756 ≈ 0.628\n",
    "      - softmax(z, 2) = ez² / Σ ≈ 2.718 / 11.756 ≈ 0.231\n",
    "      - softmax(z, 3) = ez³ / Σ ≈ 1.649 / 11.756 ≈ 0.140\n",
    "   - 따라서 클래스 \"cinema\"의 확률은 약 62.8%, \"music\"의 확률은 약 23.1%, \"science\"의 확률은 약 14.0%이다.\n",
    "   - 세 확률의 합은 1이 된다.\n",
    "      - 0.628 + 0.231 + 0.140 ≈ 0.999 (반올림으로 인해 약간의 오차가 발생할 수 있음)\n",
    "\n",
    "18. 신경망의 소프트맥스 출력은 모두 더하면 1이고 클래스 가능도와 닮았지만, 진정한 통계적 확률보다는 '확률 점수'로 이해하는 것이 낫다.\n",
    "   - 신경망이 출력하는 값은 모델이 각 클래스에 대해 얼마나 확신하는지를 나타내는 점수이다.\n",
    "   - 이 점수는 실제 확률과 다를 수 있다.\n",
    "   - 예를 들어, 모델이 클래스 A에 대해 0.9의 확률을 출력했더라도, 실제로는 클래스 A가 정답일 확률이 0.7일 수 있다.\n",
    "   - 따라서 소프트맥스 출력은 모델의 신뢰도를 나타내는 지표로 사용해야 한다.\n",
    "\n",
    "19. 크로스 엔트로피 손실은 예측 확률이 정답 분포와 얼마나 잘 맞는지를 측정한다.\n",
    "   - 정답 분포는 일반적으로 원핫 벡터(one-hot vector)로 표현된다.\n",
    "      - 원핫 벡터는 정답 클래스에 1을 할당하고 나머지 클래스에는 0을 할당하는 벡터이다.\n",
    "   - 클래스 1 = [1, 0, 0], 클래스 2 = [0, 1, 0], 클래스 3 = [0, 0, 1]로 표현된다.\n",
    "   - 하나의 크로스 엔트로피 손실은 다음과 같이 계산한다.\n",
    "      - loss(ỹ, y) = - Σⱼ₌₁ᴰ yⱼ log(ỹⱼ)\n",
    "         - ỹ: 신경망의 소프트맥스 출력 확률 벡터\n",
    "         - y: 정답 원핫 벡터\n",
    "         - D: 클래스의 개수\n",
    "         - log: 자연로그\n",
    "         - yⱼ: 정답 원핫 벡터의 j번째 요소\n",
    "         - ỹⱼ: 신경망의 소프트맥스 출력 확률의 j번째 요소\n",
    "         - y가 원핫 인코딩되어 있으므로 정답 클래스에 해당하는 원소만 덧셈 계산에 포함된다.\n",
    "            - 하나의 원소만 남겨 이 덧셈을 단순화할 수 있다.\n",
    "      - 정답 클래스가 c라고 가정하면 yc = 1이고 k=!c인 모든 경우에 yk = 0이 된다.\n",
    "         - 따라서 크로스 엔트로피 손실은 다음과 같이 단순화된다.\n",
    "            - loss(ỹ, y) = - log(ỹc)\n",
    "         - 간소화된 이 식은 손실이 정답 클래스에 할당된 확률의 음의 로그에 해당하는 것을 보여준다.\n",
    "      - N개의 샘플이 있는 경우 편균 손실은 다음과 같이 계산된다.\n",
    "         - Loss = (1/N) Σᵢ₌₁ᴺ log(ỹcᵢ)\n",
    "            - cᵢ: i번재 샘플에 대한 정답 클래스 인덱스\n",
    "\n",
    "20. 출력 층에 소프트맥스 함수를 사용할 때 크로스 엔트로피 손실은 신경망이 정답 클래스에는 높은 확률을 할당하고, 다른 클래스에는 낮은 확률을 할당하도록 유도한다.\n",
    "   - \"cinema\", \"music\", \"science\" 세 개의 클래스를 가진 문서 분류 예제의 경우 신경망의 세 개의 로짓을 생성한다.\n",
    "   - 이 로짓이 소프트맥스 함수를 통과하여 각 클래스에 대한 확률로 변환된다.\n",
    "   - 이 점수와 원핫 인코딩된 정답 레이블로 크로스 엔트로피 손실이 계산된다.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86c6414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch, torch.nn as nn\n",
    "\n",
    "# 랜덤 시드를 설정하여 일정한 결과를 얻도록 함\n",
    "#   - 재현성 보장\n",
    "#      - 성능에 변화가 있다면 난수가 아니라 코드나 하이퍼 파라미터 때문임을 알 수 있음\n",
    "#      - 팀워크에서 중요하며, 동료들이 동일한 조건에서 문제를 조하사할 수 있게 한다.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 문서 데이터\n",
    "docs = [\n",
    "\"Movies are fun for everyone.\",\n",
    "\"Watching movies is great fun.\",\n",
    "\"Enjoy a great movie today.\",\n",
    "\"Research is interesting and important.\",\n",
    "\"Learning math is very important.\",\n",
    "\"Science discovery is interesting.\",\n",
    "\"Rock is great to listen to.\",\n",
    "\"Listen to music for fun.\",\n",
    "\"Music is fun for everyone.\",\n",
    "\"Listen to folk music!\"\n",
    "]\n",
    "\n",
    "# 레이블 데이터\n",
    "labels = [1, 1, 1, 3, 3, 3, 2, 2, 2, 2]  # 1: cinema, 2: music, 3: science\n",
    "\n",
    "# 클래스 수 계산\n",
    "num_class = len(set(labels))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61f86d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 단어를 소문자 화하고 단어로 분할\n",
    "def tokenize(text):\n",
    "    # \\w+ : 단어 문자를 하나 이상 매칭 (알파벳, 숫자, 밑줄)\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "# 어휘사전을 만든다.\n",
    "def get_vocabulary(texts):\n",
    "    # 문서와 정규식 표현으로 추출한 단어를 순회하면서 말뭉치를 토큰집합으로 변환한다.\n",
    "    tokens = {token for text in texts for token in tokenize(text)}\n",
    "    \n",
    "    # 각 토큰을 알파벳 순서대로 정렬하고 인덱스를 부여하여 딕셔너리로 반환한다.\n",
    "    return {word: idx for idx, word in enumerate(sorted(tokens))}\n",
    "\n",
    "vocabulary = get_vocabulary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9ab9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 특성 벡터로 변화하는 특성 추출 함수\n",
    "def doc_to_bow(doc, vocabulary):\n",
    "    # 문서를 토큰화한다.\n",
    "    tokens = set(tokenize(doc))\n",
    "    \n",
    "    # 단어 집합 크기만큼 0으로 초기화된 벡터 생성\n",
    "    bow = [0] * len(vocabulary)  \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            bow[vocabulary[token]] = 1  # 단어가 존재하면 해당 인덱스를 1로 설정\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4135cd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 26])\n"
     ]
    }
   ],
   "source": [
    "# (10, 26) 크기의 특성 행렬 생성\n",
    "#   - 10개의 문서, 26개의 어휘사전 토큰 크기\n",
    "vectors = torch.tensor([doc_to_bow(doc, vocabulary) for doc in docs], dtype=torch.float)\n",
    "print(vectors.shape)  # 특성 행렬의 크기 출력\n",
    "\n",
    "# 원핫 인코딩 벡터가 아니라 정수 인덱스 레이블 벡터 생성\n",
    "#   - 파이토치의 크로스 엔트로피 손실 함수가 정수 레이블을 요구하기 때문\n",
    "#   - \"-1\" 빼는 이유는 파이토치 모델과 크로스엔트로피 함수가 0부터 시작하는 레이블을 요구하기 때문\n",
    "labels = torch.tensor(labels, dtype=torch.long)-1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02eb6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocabulary)  # 입력 차원 (어휘사전 크기)\n",
    "hidden_dim = 50              # 은닉층 차원\n",
    "output_dim = num_class       # 출력 차원 (클래스 수)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # 첫 번째 완전 연결 층\n",
    "        self.relu = nn.ReLU()                        # ReLU 활성화 함수\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim) # 두 번째 완전 연결 층\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 두 개의 층으로 구성된 피드포워드 신경망 구성\n",
    "        out = self.fc1(x)      # 입력을 첫 번째 층에 통과\n",
    "        out = self.relu(out)   # ReLU 활성화 함수 적용\n",
    "        out = self.fc2(out)    # 두 번째 층에 통과\n",
    "        return out  \n",
    "\n",
    "# 포워드 패스\n",
    "#   - (10, 20)크기의 입력 x가 첫 번째 완전 연결층을 통과하여 (10, 50) 크기의 은닉층 출력 생성\n",
    "#   - ReLU 활성화 함수를 통과하고 크기는 동일하게 (10, 50) 유지\n",
    "#   - 두 번째 완전 연결층을 통과하여 (10, 3) 크기의 최종 출력 생성    \n",
    "# 최종 소프트맥스 층이 생략된 이유는 파이토치의 크로스 엔트로피 손실 함수가 내부적으로 소프트맥스 함수를 포함하고 있기 때문\n",
    "model = SimpleClassifier(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4017b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [500/3000], Loss: 0.0029\n",
      "Step [1000/3000], Loss: 0.0007\n",
      "Step [1500/3000], Loss: 0.0003\n",
      "Step [2000/3000], Loss: 0.0001\n",
      "Step [2500/3000], Loss: 0.0001\n",
      "Step [3000/3000], Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # 크로스 엔트로피 손실 함수 정의\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저 정의\n",
    "\n",
    "for step in range(3000):\n",
    "    # 모델의 예측값 계산\n",
    "    outputs = model(vectors)\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 기울기 초기화\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 역전파 수행\n",
    "    loss.backward()\n",
    "    \n",
    "    # 가중치 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step+1) % 500 == 0:\n",
    "        print(f'Step [{step+1}/3000], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132b4c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music\n",
      "science\n",
      "Document: \"Listening to rock music is fun.\" => Predicted class: music\n",
      "Document: \"I love science very much.\" => Predicted class: science\n"
     ]
    }
   ],
   "source": [
    "new_docs = [\n",
    "    \"Listening to rock music is fun.\",\n",
    "    \"I love science very much.\"\n",
    "]\n",
    "\n",
    "class_names = ['cinema', 'music', 'science']\n",
    "\n",
    "# 새로운 문서를 특성 벡터로 변환\n",
    "new_vectors = torch.tensor([doc_to_bow(doc, vocabulary) for doc in new_docs], dtype=torch.float)\n",
    "\n",
    "# 새로운 문서에 대한 예측 수행\n",
    "#   - 기울기 계산이 필요 없으므로 torch.no_grad() 컨텍스트 매니저 사용\n",
    "#   - 훈련이 아니기 때문에 메모리 사용량과 계산 속도 최적화\n",
    "with torch.no_grad():\n",
    "    # 모델에 새로운 벡터 입력하여 예측값 계산\n",
    "    #   - 모든 입력을 동시에 처리한다.\n",
    "    #   - 이런 병렬 처리 방식은 벡터화된 연산을 활용하며, 입력을 하난씩 처리하는 것보다 훨씬 효율적이다.\n",
    "    outputs = model(new_vectors)\n",
    "    predicted_ids = torch.argmax(outputs, dim=1)+1  # 가장 높은 점수를 가진 클래스 인덱스 선택\n",
    "    print(class_names[predicted_ids[0].item()-1])\n",
    "    print(class_names[predicted_ids[1].item()-1])\n",
    "    \n",
    "    for i, now_doc  in enumerate(new_docs):\n",
    "        print(f'Document: \"{now_doc}\" => Predicted class: {class_names[predicted_ids[i].item()-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf0d62",
   "metadata": {},
   "source": [
    "21. BoW 방식은 간단하고 실용적이지만 한계점이 있다.\n",
    "    - 토큰의 순서나 맥락을 포착하지 못한다.\n",
    "        - 예를 들어, \"The cat sat on the mat.\"과 \"On the mat sat the cat.\"은 동일한 BoW 표현을 가지지만, 문장의 의미는 다를 수 있다.\n",
    "        - n-그램(n-gram)은 이런 문제를 해결하기 위한 한가지 방법이다.\n",
    "            - n-그램은 연속된 n개의 토큰 시퀀스를 캡처한다.\n",
    "            - 예를 들어, 2-그램(바이그램)은 \"The cat\", \"cat sat\", \"sat on\"과 같은 토큰 쌍을 포함한다.\n",
    "            - n-그램을 사용하면 토큰 간의 일부 순서 정보를 포착할 수 있지만, 여전히 문맥과 장기 의존성을 완전히 포착하지는 못한다.\n",
    "            - 사용하는데 어휘사전 크기가 기하급수적으로 증가하는 단점이 있다.\n",
    "    - 어휘사전에 없는 단어를 처리하지 못한다.\n",
    "        - 새로운 단어가 나타나면 어휘사전을 업데이트하고 모델을 재훈련해야 한다.\n",
    "    - 동의어와 유사어를 처리하지 못한다.\n",
    "        - 예를 들어, \"car\"와 \"automobile\"은 동일한 의미를 가지지만, BoW 표현에서는 별개의 토큰으로 취급된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fe52b4",
   "metadata": {},
   "source": [
    "## Chapter 2_2 단어 임베딩 <a class=\"anchor\" id=\"chapter2_2\"></a>\n",
    "1. 문서 3 \"Enjoy a great movie today.\"의 BoW 표현을 원핫 벡터로 분해할 수 있다.\n",
    "    - 문서의 BoW 벡터는 개별 단어를 나타내는 원핫 벡터의 합이다.\n",
    "     \n",
    "    ![Document-Term Matrix Example](image/02-02-doc-term-matrix-example.png)\n",
    "\n",
    "2. \"Films are my passion.\"문장에 대한 원핫 벡터와 BoW 벡터는 아래와 같다.\n",
    "\n",
    "    ![One-Hot and BoW Vectors](image/02-02-one-hot-bow-vectors.png)\n",
    "\n",
    "3. 두 가지 중요한 문제점이 있다.\n",
    "    - 단어가 훈련 데이터와 어휘 사전에 있더라도 원핫 인코딩은 이를 1이 하나 들어 있는 영벡터로 축소시켜버린다.\n",
    "        - 모델이 학습할 수 있는 의미 있는 정보를 거의 제공하지 않는다.\n",
    "    - 어휘 사전에 없는 단어는 원핫 인코딩할 수 없다.\n",
    "        - 예를 들어 \"Films\"라는 단어는 어휘 사전에 없으므로 영벡터로 표현되어 아무런 정보도 제공하지 않는다.\n",
    "\n",
    "4. 모델이 훈련 시에 본 적이 없지만 \"films\"가 \"movies\"와 의미를 공유한다는 것을 이해하게 하려면 단어 임베딩(word embedding)을 사용해야 한다.\n",
    "    - 단어 임베딩은 단어를 밀집 벡터(dense vector)로 매핑하는 방법이다.\n",
    "    - 단어 임베딩은 코사인 유사도(cosine similarity)를 통해 단어 간의 의미적 유사성을 포착할 수 있다.\n",
    "    - 예를 들어, \"movies\"와 \"films\"는 유사한 의미를 가지므로 이들의 임베딩 벡터도 유사해야 한다.\n",
    "    - 이런 데이터셋은 수백 만 개에서 수억 개의 문서로 구성된다.\n",
    "\n",
    "5. word2vec은 널리 사용되는 임베딩 학습 알고리즘으로, 스킵 그램 방식과 연속된 단어 예측(CBOW, Continuous Bag of Words) 방식의 두 가지 변형이 있다.\n",
    "    - 스킵 그램 방식은 주변 단어를 사용해 중심 단어를 예측하는 반면, CBOW 방식은 중심 단어를 사용해 주변 단어를 예측한다.\n",
    "    - 두 방식 모두 대규모 말뭉치에서 단어 임베딩을 학습하는 데 효과적이다.\n",
    "    - 스킵 그램은 한단어가 빠진 단어 시킨스이다.\n",
    "       - 예를 들어 \"Professor Alan Turing's * advanced computer science\"에서 \"*\" 누락된 단어는 \"research\", \"work\", \"theories\" 등이 될 수 있다.\n",
    "       - 이런 단어는 동의어는 아니지만 문맥상 유사한 의미를 가진 단어들이다.\n",
    "       - 이런 누락된 단어를 주변 문맥을 통해 예측하도록 모델을 훈련시키면 단어 사이의 의미 관계를 학습하는 데 도움이 된다.\n",
    "       - 이 과정을 역으로 수행하여 누락된 단어를 사용해 문맥 단어를 예측하는 것이 스킵 그램 알고리즘의 기본 작동방식이다.\n",
    "       - 스킵 그램 크기는 문맥 단어에 포함시킬 단어의 개수를 지정한다.\n",
    "       - 예를들어 크기가 5라면 누락된 단어 전과 후에 두개의 단어가 있다는 의미이고 예시는 아래의 그림과 같다.\n",
    "        \n",
    "            ![Skip-Gram Example](image/02-02-skip-gram-example.png)\n",
    "\n",
    "6. 말뭉치 어휘사전에 10,000개의 단어가 있고, 3,000개의 유닛을 가진 임베딩 층으로 구성된 스킵 그램 모델은 다음 그림과 같다.\n",
    "    - 이 스킵 그램 모델은 스킵 그램 크기가 5이고 임베딩 층의 유닛 개수가 3,000이다.\n",
    "    - 원핫 인코딩된 누락된 단어를 연속된 두 개의 완전 연결 층에 통과시켜 문맥 벡터를 예측한다.\n",
    "    - 한번에 모든 문맥 벡터를 예측하지 않고 각 단어마다 별도로 예측이 이루어진다.\n",
    "\n",
    "        ![Skip-Gram Model](image/02-02-skip-gram-model.png)\n",
    "\n",
    "    - 스킵 그램이 \"Professor Alan * research advanced\"이고 누락된 단어가 \"Turing's\"라고 가정한다.\n",
    "        - 스킵 그램이 네 개의 훈련 쌍으르 변경된다.(입력, 타깃)\n",
    "            - (\"Turing's\", \"Professor\")\n",
    "            - (\"Turing's\", \"Alan\")\n",
    "            - (\"Turing's\", \"research\")\n",
    "            - (\"Turing's\", \"advanced\")\n",
    "        - (\"Turing's\", \"Professor\")를 예로 들어 설명한다.\n",
    "            - 원핫 인코딩된 \"Turing's\" 단어가 임베딩 층을 통과하여 3,000차원의 임베딩 벡터를 생성한다.\n",
    "            - 이 임베딩 벡터가 첫 번째 완전 연결 층을 통과하여 300 유닛의 은닉 표현을 생성한다.\n",
    "            - 은닉 표현이 두 번째 완전 연결 층을 통과하여 10,000개의 유닛을 가진 출력 로짓 벡터를 생성한다.\n",
    "            - 소프트맥스 활성화 함수가 출력 로짓 벡터에 적용되어 각 어휘사전 단어에 대한 확률 분포를 생성한다.\n",
    "                - 로짓 벡터에 있는 값은 어휘사전에 있는 각각의 단어가 \"Professor\" 단어일 확률을 나타낸다.\n",
    "            - 크로스 엔트로피 손실이 계산되어 예측된 확률 분포와 원핫 인코딩된 \"Professor\" 단어 간의 차이를 측정한다.\n",
    "\n",
    "7. 훈령 쌍에서 입력(\"Turing's\")이 동일한데 출력이 왜 다른가?\n",
    "    - 입력이 동일하다면 실제 출력도 동일하지만, 문맥 단어에 따라 손실이 달라진다.\n",
    "    - 이 스킵 그램 모델은 크로스엔트로피 손실함수를 사용한다.\n",
    "    - 모델은 훈련 세트에 있는 스킵 그램에 대해 문맥 단어마다(즉 \"Turing's\" 주변에 있는 네 개의 단어마다) 개별적으로 손실을 계산한다.\n",
    "    - 이 손실을 평균하여 모든 문맥 단어 예측에 대한 피드백을 동시에 받는다.\n",
    "    - 동일한 입력을 가진 단어 쌍을 사용하더라도 모델이 의미 있는 단어 관계를 포착할 수 있도록 해준다.\n",
    "\n",
    "8. 채팅 언어 모델을 사용하다 보면 동일한 질문에도 다른 답을 얻는 경우가 많다.\n",
    "    - 모델이 비결정적이라는것을 의미하지만 정확한 말은 아니다.\n",
    "    - LLM은 기본적으로 스킵 그램 모델과 비슷하지만, 파라미터가 아주 많은 신경망 모델이다.\n",
    "    - 이런 모델의 무작위성은 텍스트를 생성하는 방식에서 비롯된다.\n",
    "    - 텍스트 생성 시에 예측 확률을 기반으로 단어를 샘플링한다.\n",
    "    - 높은 확률을 가진 단어가 선택될 가능성이 높지만 낮은 확률을 가진 단어도 선택될 수 있다.\n",
    "    - 이런 샘플링 과정이 응답의 당양성을 만든다.\n",
    "\n",
    "10. 입력 단어 \"Turing's\"에 대해 모델이 어휘사전에 잇는 단어에 다음과 같이 확률을 할당했다고 가정한다.\n",
    "    - \"Professor\": 0.1, Alan\": 0.15, \"research\": 0.2, \"advanced\": 0.05\n",
    "    - 모델을 훈력할 때 각 입력-타깃 단어 쌍이 손실에 기여한다.\n",
    "    - 훈련 데이터에서 \"Turing's\"이 \"professor\"와 함께 나타나는 경우 손실은 0.1의 점수를 높이도록 동작한다.\n",
    "        - \"Turing's\"이 \"Alan\"과 함께 나타나는 경우 손실은 0.15의 점수를 높이도록 동작한다.\n",
    "        - \"Turing's\"이 \"research\"와 함께 나타나는 경우 손실은 0.2의 점수를 높이도록 동작한다.\n",
    "        - \"Turing's\"이 \"advanced\"와 함께 나타나는 경우 손실은 0.05의 점수를 높이도록 동작한다.\n",
    "    - 역전파 과정에서 모델은 주어진 문맥에 대해 확률 점수가 높아지도록 가중치를 조절한다.\n",
    "        - 예를 들어 업데이트 된 점수는 다음과 같을 수 있다.\n",
    "        - \"Professor\": 0.12, Alan\": 0.18, \"research\": 0.25, \"advanced\": 0.07\n",
    "        - 반면 어휘사전에 있는 다른 단어의 점수는 조금 감소한다.\n",
    "\n",
    "11. 훈련이 완료되면 출력 층은 버려지고 임베딩 층이 새로운 출력 층으로 사용된다.\n",
    "    - 원핫 인코딩된 입력 단어가 주어지면 모델이 300차원의 벡터를 생성하는데, 이를 단어 임베딩이라고 한다.\n",
    "\n",
    "12. word2vec은 레이블이 없는 대규모 텍스트 말뭉치에서 단어 임베딩을 학습하는 하나의 방법일 뿐이다.\n",
    "\n",
    "13. 단어 임베딩을 사용해 텍스트를 표현하는 것은 BoW에 비해 명확한 이점이 있다.\n",
    "    - 차원 축소\n",
    "        - 단어 임베딩은 일반적으로 수백 차원의 밀집 벡터로 표현되므로, BoW의 희소하고 고차원적인 표현에 비해 훨씬 더 효율적이다.\n",
    "    - 의미적 유사성\n",
    "        - 비슷한 의미를 가진 단어는 임베딩 공간에서 서로 가까이에 위치한 벡터로 매핑된다.\n",
    "        - 구글이 약 1천억 개의 단어가 표함된 뉴스 말뭉치에서 훈련한 단어 임베딩을 사용한 예시를 살펴보자.\n",
    "            - \"Moscow\", \"Beiging\", \"Tokyo\" 단어는 모두 수도를 나타내므로 임베딩 공간에서 서로 가깝게 위치한다.\n",
    "            - 단어 사이의 의미적 관계가 반영되어 있다.\n",
    "            - 아래의 그림은 국가와 수도에 대한 300차원의 word2vec 임베딩을 2차원으로 축소하여 시각화한 것이다.\n",
    "            - 의미가 있는 관련되어 있는 단어는 서로 군집을 이루고 있고 국가와 해당 국가의 도시를 연결하는 선은 거의 평행하여, 의미적 관계를 보여준다.\n",
    "\n",
    "                ![Word Embedding Example](image/02-02-word-embedding-example.png)\n",
    "\n",
    "14. 스킵 그램 모델은 두 단어가 직접적으로 동시에 등장하지 않더라도 미슷한 문맥에 등장할 때 의미적 유사성을 포착합니다.\n",
    "    - 예를 들어 \"doctor\"와 \"physician\" 단어는 동의어이지만, 동일한 문장에서 자주 함께 등장하지는 않는다.\n",
    "    - 그러나 두 단어는 유사한 문맥에서 자주 등장한다.\n",
    "    - 스킵 그램 모델은 이런 문맥 정보를 활용하여 두 단어의 임베딩 벡터가 서로 가깝게 위치하도록 학습할 수 있다.\n",
    "    - 결과적으로 \"doctor\"와 \"physician\" 단어는 임베딩 공간에서 서로 가까운 벡터로 매핑된다.\n",
    "\n",
    "15. 300차원의 벡터를 시각화 하는 것은 불가능하므로 주성분 분석(Principal Component Analysis, PCA)같은 차원 축소 기법을 사용해 벡터를 2차원 공간에 투영시켰다.\n",
    "    - 이 두 차원을 각각 첫 번째 주성분과 두 번째 주성분이라고 부른다.\n",
    "    - 차원 축소 알고리즘은 고차원 벡터의 관계를 유지하면서 벡터를 압축한다.\n",
    "    - 위의 그개프에서 첫 번째 주성분과 두 번째 주성분은 단어 사이의 의미적 연결을 보존하여 단어 사이의 관계가 잘 들어난다.\n",
    "\n",
    "16. 단어 임베딩은 단어의 의미와 다른 단어와의 관계를 포착한다.\n",
    "    - 신경망 모델에서 문서를 단어의 임베딩 행렬로 인코딩한다.\n",
    "    - 이 행렬의 각 행은 단어의 임베딩 벡터이며, 행의 위치는 단어가 문서에 등장하는 위치가된다.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cba72d",
   "metadata": {},
   "source": [
    "## Chapter 2_3 바이트 페어 인코딩 <a class=\"anchor\" id=\"chapter2_3\"></a>\n",
    "1. 바이트 페어 인코딩은 단어를 부분단어라 부르는 작은 단위로 나누어 어휘사전에 없는 단어 처리 문제를 해결한 토큰화 알고리즘이다.\n",
    "    - 데이터 압축 기법으로 고안되었지만 단어를 문자의 시퀸스로 처리하는 식으로 BPE를 자연어 처리에 적용할 수 있다.\n",
    "    - 가장 자주 등장하는 기호(문자나 부분단어) 쌍을 새로운 부분단어로 합친다.\n",
    "    - 목표한 크기의 어휘사전에 도달할 때 까지 계속된다.\n",
    "\n",
    "2. 기본적은 BPE 알고리즘은 다음과 같다.\n",
    "    - 인접한 기호 쌍을 카운트한다.\n",
    "        - 각 문자를 하나의 기호로 다룬다.\n",
    "        - 말뭉치에서 인접한 기호 쌍을 모두 카운트 한다.\n",
    "        - \"hello\"에는 \"h e\", \"e l\", \"l l\", \"l o\" 네 개의 인접한 기호 쌍이 있다.\n",
    "    - 가장 빈도가 높은 기호 쌍을 선택한다.\n",
    "        - \"l l\"이 가장 자주 등장한다면 이 쌍을 선택한다.\n",
    "    - 선택된 기호 쌍을 새로운 병합 기호로 교체한다.\n",
    "        - \"l l\" 쌍을 \"ll\"이라는 새로운 기호로 교체한다.\n",
    "        - \"hello\"는 이제 \"h e ll o\"로 표현된다.\n",
    "    - 어휘사전을 업데이트한다.\n",
    "        - \"_ h e l l o\" -> \"_ h e ll o\"\n",
    "                - 반복 후 마지막에서는 \"_ h e l l o\" -> \"_hello\"가 된다.\n",
    "        - 기호 쌍 카운트 시 .split()에서 나누어지지 않아 대상에서 제외된다.\n",
    "    - 토큰 집합에 새로운 기호를 추가한다.\n",
    "        - 기호 \"ll\"을 토큰 집합에 추가한다.\n",
    "    - 이 과정을 목표한 토큰 집합 크기에 도달할 때까지 반복한다.\n",
    "        - 초기 어휘사전 단어의 공백이 사라지면 그 단어에 대한 작업이 끝나고, 다음 단어로 넘어간다.\n",
    "\n",
    "3. 기호 쌍을 반복해서 카운트하고 병합 후에 전체 말뭉치를 업데이느하는 데 계산 비용이 많이든다.\n",
    "\n",
    "4. 좀더 효율적인 방법은 다음과 같다.\n",
    "    - 말뭉치에 있는 고유한 모든 단어와 등장 횟수로 어휘사전을 초기화한다.\n",
    "    - 이 단어 카운트를 사용해 토큰 쌍을 카운트를 계산한다.\n",
    "    - 가장 자주 등장하는 토큰 쌍을 병합하는 식으로 어휘사전을 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ce42f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "단어를 문자로 분할하여 말뭉치에서 초기 어휘사전을 만듭니다.\n",
    "단어 경계 문자 '_'를 추가하고 고유한 문자를 추적합니다.\n",
    "\n",
    "매개변수:\n",
    "    corpus (iterable): 처리할 단어의 반복자 또는 리스트\n",
    "\n",
    "반환값:\n",
    "    tuple: (토큰화된 단어를 카운트에 매핑하는 어휘사전 딕셔너리, 말뭉치에 있는 고유한 문자 집합)\n",
    "\"\"\"\n",
    "def initialize_vocabulary(corpus):\n",
    "    # vocabulary: 토큰화된 단어 시퀸스의 출현 빈도를 저장하는 딕셔너리\n",
    "    # defaultdict(int): 기본값이 int인 딕셔너리\n",
    "    #   - key가 없을 때 자동으로 0으로 초기화\n",
    "    vocabulary = defaultdict(int)\n",
    "    \n",
    "    # charset: 고유한 문자들의 집합\n",
    "    charset = set()\n",
    "    for word in corpus:\n",
    "        # 단어 앞에 특수 마커 \"_\"를 추가하여 단어의 시작을 표시\n",
    "        #   - \"restart\"의 \"_re\"와 \"agree\"의 \"re\"는 다르다는 것을 구분하기 위함\n",
    "        #   - 모델이 생성한 토큰에서 문장을 재구성하는 데 도움이 된다.\n",
    "        #   - 토큰이 \"_\"로 시작하면 새로운 단어를 의미하며 토큰 앞에 공백을 추가해야한다.\n",
    "        word_with_marker = \"_\"+word\n",
    "        \n",
    "        # 각 단어를 개별 문자로 분할한다.\n",
    "        #   - 예: \"hello\" -> ['h', 'e', 'l', 'l', 'o']\n",
    "        characters = list(word_with_marker)\n",
    "        \n",
    "        # 고유한 문자 집합에 현재 단어의 문자들을 추가한다.\n",
    "        charset.update(characters)\n",
    "        \n",
    "        # 문자를 공백으로 연결하여 토큰화된 단어 시퀸스를 생성한다.\n",
    "        #   - 예: \"hello\" -> \"h e l l o\"\n",
    "        tokenized_word = ' '.join(characters)\n",
    "        \n",
    "        # tokenized_word를 어휘사전에 추가하고 출현 빈도를 1 증가시킨다.\n",
    "        vocabulary[tokenized_word] +=1\n",
    "    return vocabulary, charset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "26e00bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary :  defaultdict(<class 'int'>, {'_ r e s t a r t': 1, '_ a g r e e': 1, '_ h e l l o': 1})\n",
      "charset :  {'g', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', 'r'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary, charset = initialize_vocabulary([\"restart\", \"agree\",\"hello\"])\n",
    "print(\"vocabulary : \",vocabulary)\n",
    "print(\"charset : \", charset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a161eff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\"\"\"\n",
    "어휘사전에 인접한 기호 쌍의 빈도를 카운트합니다.\n",
    "가장 빈번한 쌍을 식별하여 합치는데 사용합니다.\n",
    "\n",
    "매개변수:\n",
    "    vocabulary (dict): 토큰화된 단어와 카운트를 매핑하는 딕셔너리\n",
    "\n",
    "Returns:\n",
    "    defaultdict: 토큰 쌍과 빈도 카운트의 매핑\n",
    "\"\"\"\n",
    "def get_pair_counts(vocabulary):\n",
    "    # 쌍의 출현 빈도를 저장하는 딕셔너리\n",
    "    pair_counts = defaultdict(int)\n",
    "    \n",
    "    # 어휘사전의 각 토큰화된 단어와 그 출현 빈도를 순회\n",
    "    #   - vocabulary :  defaultdict(<class 'int'>, {'_ r e s t a r t': 1, '_ a g r e e': 1, '_ h e l l o': 1})\n",
    "    for tokenized_word, count in vocabulary.items():\n",
    "        #print('tokenized_word: ', tokenized_word, ' / count :',count)\n",
    "        # 토큰화된 단어를 개별 토큰으로 분할\n",
    "        tokens = tokenized_word.split()\n",
    "        #print('tokens: ', tokens)\n",
    "        # 인접한 토큰 쌍을 순회\n",
    "        for i in range(len(tokens)-1):\n",
    "            pair = (tokens[i], tokens[i+1])  # 인접한 두 토큰으로 쌍 생성\n",
    "            #print('pair: ', pair)\n",
    "            # 해당 쌍은 기본적으로 Vocabulary에서의 단어 출현 빈도만큼 증가\n",
    "            #   - 예: \"h e l l o\"가 2번 등장하면 쌍 (\"h\", \"e\")도 2번 등장\n",
    "            pair_counts[pair] += count         \n",
    "            #print('pair_counts[pair]: ', pair_counts[pair])\n",
    "    return pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "63cbd1ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pair_counts :  defaultdict(<class 'int'>, {('_', 'r'): 1, ('r', 'e'): 2, ('e', 's'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'r'): 1, ('r', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 'r'): 1, ('e', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n"
     ]
    }
   ],
   "source": [
    "pair_counts = get_pair_counts(vocabulary)\n",
    "print(\"pair_counts : \", pair_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd5e5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\"\"\"\n",
    "어휘사전에 있는 특정 기호 쌍을 합칩니다.\n",
    "토큰 경계를 정확하게 매치하기 위해 정규 표현식을 사용합니다.\n",
    "\n",
    "매개변수:\n",
    "vocab (dict): 현재 어휘사전 딕셔너리\n",
    "pair (tuple): 병합된 토큰 쌍\n",
    "\n",
    "반환값:\n",
    "dict: 특정 쌍이 병합된 새로운 어휘사전\n",
    "\"\"\"\n",
    "def merge_pair(vocab, pair):\n",
    "    new_vocabulary = {}\n",
    "    # 문자열에 있는 특수문자에 대해 자동으로 역슬래쉬 문자를 추가한다.\n",
    "    #  - 예: ('h', 'e') -> 'h\\ e'\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    #print('bigram : ', bigram)\n",
    "    # 정규식 패턴 생성\n",
    "    #   - (?<!\\S): 앞에 공백이 없음을 의미 (단어의 시작)\n",
    "    #   - (?!\\S): 뒤에 공백이 없음을 의미 (단어의 끝)\n",
    "    #   - 온전한 토큰 쌍에만 매칭된다. \n",
    "    #   - 예: \"good morning\"은 \"this is good morning\"에는 매칭되지만 \"this isgood morning\"에는 매칭되지 않음\n",
    "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for tokenized_word, count in vocab.items():\n",
    "        # text 중 pattern에 해당하는 부분을 replace로 대체한다.\n",
    "        #   - 예: \"h e l l o\"에서 (\"h\", \"e\")를 병합하면 \"he l l o\"가 됨\n",
    "        new_tokenized_word = pattern.sub(''.join(pair), tokenized_word)\n",
    "        \n",
    "        # 병합된 새로운 토큰화된 단어와 그 출현 빈도를 새로운 어휘사전에 추가\n",
    "        new_vocabulary[new_tokenized_word] = count\n",
    "    return new_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19a24a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The key with the maximum value is: b\n",
      "The maximum value is: 20\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "my_defaultdict = defaultdict(int)\n",
    "my_defaultdict['a'] = 10\n",
    "my_defaultdict['b'] = 20\n",
    "my_defaultdict['c'] = 20\n",
    "\n",
    "max_key = max(my_defaultdict, key=my_defaultdict.get)\n",
    "print(f\"The key with the maximum value is: {max_key}\")\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "my_defaultdict = defaultdict(int)\n",
    "my_defaultdict['a'] = 10\n",
    "my_defaultdict['b'] = 5\n",
    "my_defaultdict['c'] = 20\n",
    "\n",
    "max_value = max(my_defaultdict.values())\n",
    "print(f\"The maximum value is: {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6bd1c509",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "말뭉치를 처리하여 토크나이저에 필요한 구성요소를 만든다.\n",
    "어휘사전과 문자 집합을 초기화하고, 목표 어휘사전 크기까지 토큰 쌍을 반복적으로 병합한다.\n",
    "\n",
    "매개변수:\n",
    "corpus (iterable): 처리할 단어의 반복자 또는 리스트\n",
    "vocab_size (int): 목표 어휘사전 크기\n",
    "\n",
    "반환값:\n",
    "tuple: (최종 어휘사전, 병합된 토큰 쌍의 리스트, 문자 집합, 최종 토큰 집합)\n",
    "\"\"\"\n",
    "def byte_pair_encoding(corpus, vocab_size):\n",
    "    # 어휘사전과 문자 집합 초기화\n",
    "    vocabulary, charset = initialize_vocabulary(corpus)\n",
    "    print(\"Initial vocabulary : \", vocabulary)\n",
    "    print(\"Initial charset : \", charset)\n",
    "    \n",
    "    # 병합된 토큰 쌍을 저장할 리스트 초기화\n",
    "    merges = []\n",
    "    \n",
    "    # tokens를 초기 문자집합으로 설정\n",
    "    tokens = set(charset)\n",
    "    \n",
    "    # 어휘사전 크기가 목표 어휘사전 크기보다 작을 때까지 반복\n",
    "    #   - 토큰 개수가 vocab_size에 도달할 때까지 반복\n",
    "    while len(tokens) < vocab_size:\n",
    "        # 현재 어휘사전에서 토큰 쌍의 출현 빈도 계산\n",
    "        pair_counts = get_pair_counts(vocabulary)\n",
    "        print(\"Current pair_counts : \", pair_counts)\n",
    "        \n",
    "        if not pair_counts:\n",
    "            break  # 더 이상 병합할 쌍이 없으면 종료\n",
    "        \n",
    "        # 가장 빈도가 높은 토큰 쌍 선택\n",
    "        most_frequent_pair = max(pair_counts, key=pair_counts.get)\n",
    "        print(\"Most frequent pair : \", most_frequent_pair)\n",
    "        \n",
    "        # 선택된 쌍을 병합 리스트에 추가\n",
    "        merges.append(most_frequent_pair)  \n",
    "        print(\"Merges so far : \", merges)\n",
    "        \n",
    "        # 선택된 쌍을 어휘사전에서 병합\n",
    "        vocabulary = merge_pair(vocabulary, most_frequent_pair)\n",
    "        print(\"Updated vocabulary : \", vocabulary)\n",
    "        \n",
    "        # 새로운 토큰 생성\n",
    "        new_token = ''.join(most_frequent_pair)\n",
    "        print(\"New token : \", new_token)\n",
    "        \n",
    "        # 새로운 토큰을 토큰 집합에 추가\n",
    "        tokens.add(new_token)  \n",
    "        print(\"Updated tokens : \", tokens)\n",
    "    \n",
    "    return vocabulary, merges, charset, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "966afbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary :  defaultdict(<class 'int'>, {'_ r e s t a r t': 1, '_ a g r e e': 1, '_ h e l l o': 1})\n",
      "Initial charset :  {'g', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_', 'r'): 1, ('r', 'e'): 2, ('e', 's'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'r'): 1, ('r', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 'r'): 1, ('e', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('r', 'e')\n",
      "Merges so far :  [('r', 'e')]\n",
      "Updated vocabulary :  {'_ re s t a r t': 1, '_ a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  re\n",
      "Updated tokens :  {'g', 're', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_', 're'): 1, ('re', 's'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'r'): 1, ('r', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_', 're')\n",
      "Merges so far :  [('r', 'e'), ('_', 're')]\n",
      "Updated vocabulary :  {'_re s t a r t': 1, '_ a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  _re\n",
      "Updated tokens :  {'g', 're', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', '_re', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_re', 's'): 1, ('s', 't'): 1, ('t', 'a'): 1, ('a', 'r'): 1, ('r', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_re', 's')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's')]\n",
      "Updated vocabulary :  {'_res t a r t': 1, '_ a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  _res\n",
      "Updated tokens :  {'_res', 'g', 're', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', '_re', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_res', 't'): 1, ('t', 'a'): 1, ('a', 'r'): 1, ('r', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_res', 't')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't')]\n",
      "Updated vocabulary :  {'_rest a r t': 1, '_ a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  _rest\n",
      "Updated tokens :  {'_res', 'g', 're', '_rest', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', '_re', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_rest', 'a'): 1, ('a', 'r'): 1, ('r', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_rest', 'a')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a')]\n",
      "Updated vocabulary :  {'_resta r t': 1, '_ a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  _resta\n",
      "Updated tokens :  {'_res', '_resta', 'g', 're', '_rest', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', '_re', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_resta', 'r'): 1, ('r', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_resta', 'r')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r')]\n",
      "Updated vocabulary :  {'_restar t': 1, '_ a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  _restar\n",
      "Updated tokens :  {'_res', '_restar', '_resta', 'g', 're', '_rest', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', '_re', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_restar', 't'): 1, ('_', 'a'): 1, ('a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_restar', 't')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't')]\n",
      "Updated vocabulary :  {'_restart': 1, '_ a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  _restart\n",
      "Updated tokens :  {'_res', '_restar', '_resta', 'g', 're', '_rest', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', '_re', '_restart', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_', 'a'): 1, ('a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_', 'a')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a')]\n",
      "Updated vocabulary :  {'_restart': 1, '_a g re e': 1, '_ h e l l o': 1}\n",
      "New token :  _a\n",
      "Updated tokens :  {'_res', '_restar', '_resta', '_a', 'g', 're', '_rest', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', '_re', '_restart', 'r'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_a', 'g'): 1, ('g', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_a', 'g')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g')]\n",
      "Updated vocabulary :  {'_restart': 1, '_ag re e': 1, '_ h e l l o': 1}\n",
      "New token :  _ag\n",
      "Updated tokens :  {'a', 's', '_restar', '_a', 'g', '_re', 'l', '_', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_ag', 're'): 1, ('re', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_ag', 're')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're')]\n",
      "Updated vocabulary :  {'_restart': 1, '_agre e': 1, '_ h e l l o': 1}\n",
      "New token :  _agre\n",
      "Updated tokens :  {'a', 's', '_restar', '_a', 'g', '_re', 'l', '_', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_agre', 'e'): 1, ('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_agre', 'e')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e')]\n",
      "Updated vocabulary :  {'_restart': 1, '_agree': 1, '_ h e l l o': 1}\n",
      "New token :  _agree\n",
      "Updated tokens :  {'a', 's', '_agree', '_restar', '_a', 'g', '_re', 'l', '_', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_', 'h'): 1, ('h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_', 'h')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e'), ('_', 'h')]\n",
      "Updated vocabulary :  {'_restart': 1, '_agree': 1, '_h e l l o': 1}\n",
      "New token :  _h\n",
      "Updated tokens :  {'a', 's', '_agree', '_restar', '_a', 'g', '_re', 'l', '_', '_h', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_h', 'e'): 1, ('e', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_h', 'e')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e'), ('_', 'h'), ('_h', 'e')]\n",
      "Updated vocabulary :  {'_restart': 1, '_agree': 1, '_he l l o': 1}\n",
      "New token :  _he\n",
      "Updated tokens :  {'a', 's', '_agree', '_restar', '_he', '_a', 'g', '_re', 'l', '_', '_h', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_he', 'l'): 1, ('l', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_he', 'l')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e'), ('_', 'h'), ('_h', 'e'), ('_he', 'l')]\n",
      "Updated vocabulary :  {'_restart': 1, '_agree': 1, '_hel l o': 1}\n",
      "New token :  _hel\n",
      "Updated tokens :  {'a', 's', '_agree', '_hel', '_restar', '_he', '_a', 'g', '_re', 'l', '_', '_h', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_hel', 'l'): 1, ('l', 'o'): 1})\n",
      "Most frequent pair :  ('_hel', 'l')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e'), ('_', 'h'), ('_h', 'e'), ('_he', 'l'), ('_hel', 'l')]\n",
      "Updated vocabulary :  {'_restart': 1, '_agree': 1, '_hell o': 1}\n",
      "New token :  _hell\n",
      "Updated tokens :  {'a', 's', '_agree', '_hel', '_restar', '_he', '_a', '_hell', 'g', '_re', 'l', '_', '_h', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {('_hell', 'o'): 1})\n",
      "Most frequent pair :  ('_hell', 'o')\n",
      "Merges so far :  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e'), ('_', 'h'), ('_h', 'e'), ('_he', 'l'), ('_hel', 'l'), ('_hell', 'o')]\n",
      "Updated vocabulary :  {'_restart': 1, '_agree': 1, '_hello': 1}\n",
      "New token :  _hello\n",
      "Updated tokens :  {'_hello', 'a', 's', '_agree', '_hel', '_restar', '_he', '_a', '_hell', 'g', '_re', 'l', '_', '_h', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n",
      "Current pair_counts :  defaultdict(<class 'int'>, {})\n",
      "Final Vocabulary:  {'_restart': 1, '_agree': 1, '_hello': 1}\n",
      "Merges:  [('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e'), ('_', 'h'), ('_h', 'e'), ('_he', 'l'), ('_hel', 'l'), ('_hell', 'o')]\n",
      "Charset:  {'g', 'h', 't', 'a', 'l', 'e', '_', 's', 'o', 'r'}\n",
      "Tokens:  {'_hello', 'a', 's', '_agree', '_hel', '_restar', '_he', '_a', '_hell', 'g', '_re', 'l', '_', '_h', 'r', '_res', '_resta', '_rest', 't', 'e', '_ag', '_agre', 're', 'h', 'o', '_restart'}\n"
     ]
    }
   ],
   "source": [
    "vocabulary, merges, charset, tokens = byte_pair_encoding([\"restart\", \"agree\",\"hello\"], vocab_size=100)\n",
    "print(\"Final Vocabulary: \", vocabulary)\n",
    "print(\"Merges: \", merges)\n",
    "print(\"Charset: \", charset)\n",
    "print(\"Tokens: \", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "340655c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte_pair_encoding 함수를 통해 얻은 merges, vocabulary, charset으로 word를 토큰화 한다.\n",
    "def tokenize_word(word, merge, vocabulary, charset, unk_token=\"<UNK>\"):\n",
    "    # 단어 앞에 특수 마커 \"_\"를 추가하여 단어의 시작을 표시\n",
    "    word = \"_\"+word\n",
    "    \n",
    "    # 단어가 어휘사전에 있으면 그대로 반환\n",
    "    if word in vocabulary:\n",
    "        return [word]  \n",
    "    \n",
    "    # word를 개별 문자로 분할하고 어휘사전에 없는 문자는 <UNK>로 대체\n",
    "    tokens = [char if char in charset else unk_token for char in word]\n",
    "    \n",
    "    for left, right in merge:\n",
    "        i = 0\n",
    "        while i < len(tokens)-1:\n",
    "            if tokens[i:i+2] == [left, right]:\n",
    "                # 인접한 토큰 쌍이 병합 리스트에 있으면 병합\n",
    "                tokens[i:i+2] = [left + right]\n",
    "            else:\n",
    "                i += 1\n",
    "    return tokens\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "84ba95b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('r', 'e'), ('_', 're'), ('_re', 's'), ('_res', 't'), ('_rest', 'a'), ('_resta', 'r'), ('_restar', 't'), ('_', 'a'), ('_a', 'g'), ('_ag', 're'), ('_agre', 'e'), ('_', 'h'), ('_h', 'e'), ('_he', 'l'), ('_hel', 'l'), ('_hell', 'o')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['_', '_res']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(merges)\n",
    "tokenize_word(\"_res\", merges, vocabulary, charset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15df5e07",
   "metadata": {},
   "source": [
    "## Chapter 2_4 언어모델 <a class=\"anchor\" id=\"chapter2_4\"></a>\n",
    "1. 언어 모델은 이전 토큰을 기반으로 조건부 확률을 추정하여 시퀸스에 있는 다음 토큰을 예측한다.\n",
    "    - 레이블링되지 않은 대규모 텍트스 말뭉치에서 훈련함으로 써 언어 모델이 언어에 있는 통계적 패턴을 학습하여 사람이 쓴 것처럼 텍스트를 생성할 수 있다.\n",
    "\n",
    "2. L개의 토큰으로 구성된 시퀸스 s에 대하여 언어 모델은 다음과 같이 계산된다..\n",
    "    - Pr(t = tₗ₊₁ | s = (t₁, t₂, ..., tₗ))\n",
    "        - Pr: 어휘사전의 토큰에 대한 조건부 확률 분포\n",
    "           - 조건부 확률은 어떤 사건 A가 사건 B에 의해 영향을 받을 때, 사건 B가 발생했을 때 사건 A가 발생할 확률을 의미한다.\n",
    "           - 언어 모델에서는 이전 토큰 시퀸스가 주어졌을 때 특정 토큰이 다음 토큰이 될 확률을 반영한다.\n",
    "           - 이 시퀸스를 입력 시퀸스, 문맥 프롬프트라고 부른다.\n",
    "        - t: 어휘사전에 있는 토큰\n",
    "        - tₗ₊₁: 시퀸스 s 다음에 오는 토큰\n",
    "        - s: 이전 L개의 토큰으로 구성된 시퀸스  \n",
    "    - Pr(tₗ₊₁ | t₁, t₂, ..., tₗ) 또는 Pr(tₗ₊₁ | s)\n",
    "    - 맥락에 맞게 간소화된 식부터 복잡한 식까지 여러 가지 형태로 표현할 수 있다.\n",
    "\n",
    "3. 모든 토큰 t와 시퀸스 s에 대해 조건부 확률은 Pr(t | s) >= 0을 만족한다.\n",
    "    - 즉, 모든 토큰과 시퀸스에 대해 조건부 확률은 음수가 될 수 없다.\n",
    "    - 어휘사전 V에 있는 모든 토큰에 대한 확률의 합은 1이어야 한다.\n",
    "        - Σₜ∈V Pr(t | s) = 1\n",
    "        - 즉, 모든 토큰에 대한 조건부 확률의 합은 1이 된다.\n",
    "\n",
    "4. 모델이 어휘사전에 대해 유효한 이산 확률분포를 출력하게 된다.\n",
    "    - 이산환률분포: 유한하거나 셀 수 있는 무한 개수의 가능한 값에 확률을 할당하는 확률의 분포이다.\n",
    "    - 5개의 단어 \"are\", \"cool\", \"lanaguage\", \"models\", \"useless\"가 포함된 어휘사전 V가 있다고 가정한다.\n",
    "    - 언어 모델은 V에 있는 가능한 다음 단어에 대한 확률을 다음과 같이 출력할 수 있다.\n",
    "        - Pr(\"are\" | s) = 0.01\n",
    "        - Pr(\"cool\" | s) = 0.77\n",
    "        - Pr(\"language\" | s) = 0.02\n",
    "        - Pr(\"models\" | s) = 0.15\n",
    "        - Pr(\"useless\" | s) = 0.05\n",
    "    - 이 확률의 합은 1이 되어 유효한 이산 확률 분포가 된다.\n",
    "        - 0.01 + 0.77 + 0.02 + 0.15 + 0.05 = 1.00\n",
    "\n",
    "5. 이런 종류의 모델을 자기회귀 언어 모델, 또는 코잘 언어 모델이라고 한다.\n",
    "    - 자귀회귀는 시퀸스에 있는 이전 원소만을 사용해 다음 원소를 예측한다.\n",
    "        - 이런 모델은 텍스트 생성에 뛰어나며, 트랜스포머 기반 채팅 언어 모델과 이책에 서 다루는 언어모델이 여기에 속한다.\n",
    "    - 트랜스포머 기번 모델인 BERT와 같은 마스크드 언어 모델은 다른 방식을 사용한다.\n",
    "        - 의도적으로 시퀸스 내에 마스킹된 토큰을 이전 문맥과 이후 문맥을 모두 사용해 예측한다.\n",
    "        - 텍스트 분류와 개체명 인식 같은 작업에 잘 맞는다.\n",
    "\n",
    "6. 신경망 언어 모델링이 표준이 되기 전에에는 통계적인 방법에 의존했다.\n",
    "    - n-그램 언어 모델은 시퀸스에 있는 이전 n-1개의 토큰을 사용해 다음 토큰을 예측한다.\n",
    "    - 예를 들어, 3-그램(트라이그램) 모델은 이전 두 개의 토큰을 사용해 다음 토큰을 예측한다.\n",
    "    - 이런 모델은 단순하고 구현이 쉽지만, 긴 문맥을 포착하는 데 한계가 있다.\n",
    "    - 또한 희귀한 n-그램 조합에 대해 일반화하는 데 어려움이 있다.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb864e1",
   "metadata": {},
   "source": [
    "## Chapter 2_5 카운트 기반 언어 모델 <a class=\"anchor\" id=\"chapter2_5\"></a>\n",
    "1. 트라이그램(trigram) 모델(n=3)을 사용해 작동 방식을 설명한다.\n",
    "    - 트라이그램 모델은 시퀸스에 있는 이전 두 개의 토큰을 사용해 다음 토큰을 예측한다.\n",
    "    - 예를 들어, 시퀸스 s = (t₁, t₂, t₃, ..., tₗ)가 주어지면 트라이그램 모델은 다음과 같이 계산된다.\n",
    "        - Pr(tₗ | tₗ₋₂, tₗ₋₁) = C(tₗ₋₂, tₗ₋₁, tₗ) / C(tₗ₋₂, tₗ₋₁)\n",
    "            - C(tₗ₋₂, tₗ₋₁, tₗ): 시퀸스 s에서 토큰 tₗ₋₂, tₗ₋₁, tₗ가 연속적으로 등장한 횟수\n",
    "            - tₗ: 시퀸스 s 다음에 오는 토큰\n",
    "            - tₗ₋₂, tₗ₋₁: 시퀸스 s에 있는 이전 두 개의 토큰\n",
    "            - 주어진 문맥에 대한 토큰 확률의 최대 가능도 추정(maximum likelihood estimation, MLE)\n",
    "                - 이전의 두 토큰이 동일한 모든 트라이그램 중에서  특정 트라이그램의 상대적인 빈도를 계산한다.\n",
    "                - 훈련 말뭉치가 커지면 MLE는 데이터에 자주 등장하는 n-그램에 더 나은 추정치를 제공한다.\n",
    "                - 데이터 셋이 클수록 더 정확한 확률 추정치를 제공한다.\n",
    "    - 트라이그램 \"laguage models rock\"이 말뭉치에서 50번 나타나고, \"language models\"가 200번 나타난다고 가정한다.\n",
    "        - Pr(\"rock\" | \"language\", \"models\") = C(\"language\", \"models\", \"rock\") / C(\"language\", \"models\") = 50 / 200 = 0.25\n",
    "        - 즉, \"language models\" 다음에 \"rock\"이 올 확률은 25%이다.\n",
    "\n",
    "2. 하지만 제한된 크기의 말뭉치가 문제가된다.\n",
    "    - 실무에서 만날 수 있는 일부 n-그램이 훈련 데이터에 없을 수 있다.\n",
    "    - 예를 들어, \"language models sing\"이 말뭉치에 없다면 MLE는 다음과 같이 계산된다.\n",
    "        - Pr(\"sing\" | \"language\", \"models\") = C(\"language\", \"models\", \"sing\") / C(\"language\", \"models\") = 0 / 200 = 0\n",
    "    - 유효한 문장이더라도 이전에 본 적 없는 n-그램을 가지고 있는 모든 시퀸에 대해 확률을 0으로 예측하게 된다.\n",
    "\n",
    "3. 이런 문제를 해결하기 위해 개발된 기법 중 하나는 백오프(backoff)이다.\n",
    "    - 고차원 n-그램(예를 들어, 트라이그램)이 관측되지 않는다면 그보다 낮은 차원의 n-그램(예를 들어, 바이그램)으로 후퇴해 확률을 추정한다.\n",
    "    - Pr(tₗ | tₗ₋₂, tₗ₋₁)는 조건에 따라서 다음 식 중 하나로 계산된다.\n",
    "        - C(tₗ₋₂, tₗ₋₁, tₗ) > 0인 경우:\n",
    "            - Pr(tₗ | tₗ₋₂, tₗ₋₁) = C(tₗ₋₂, tₗ₋₁, tₗ) / C(tₗ₋₂, tₗ₋₁)\n",
    "        - C(tₗ₋₂, tₗ₋₁, tₗ) = 0 이고 C(tₗ₋₁, tₗ) > 0인 경우:\n",
    "            - Pr(tₗ | tₗ₋₁) = C(tₗ₋₁, tₗ) / C(tₗ₋₁)\n",
    "        - Pr(tₗ) 그외\n",
    "    - 바이그램과 유니그램의 확률은 다음과 같이 계산된다.\n",
    "        - Pr(tₗ | tₗ₋₁) = C(tₗ₋₁, tₗ) / C(tₗ₋₁)\n",
    "        - Pr(tₗ) = C(tₗ) + 1 / W + V\n",
    "            - C(tₗ): 말뭉치에서 토큰 tₗ가 등장한 횟수\n",
    "            - W: 말뭉치에 있는 총 토큰 개수\n",
    "            - V: 어휘사전 크기\n",
    "    -  C(tₗ)에 더하지는 1은 애드원 스무딩(add-one smoothing) 또는 라플라스 스무딩(Laplace smoothing)이라고 부른다.\n",
    "        - 말뭉치에 토큰이 없어 확률이 0이 되는 것을 방지한다.\n",
    "        - 각 토큰에 카운트 1을 더해 이전에 본적 없는 토큰을 포함해 코든 토큰에 0이 아닌 작은 확률을 할당하여 문제를 해결한다.\n",
    "        - 분자 추가된 1을 보상하기 위해 분모에 어휘사전 크기 V를 더한다.\n",
    "\n",
    "4. 머신러닝에서 훈련 에 전체 데이터셋을 사용하면 모델이 잘 일반회되는지 평가할 수 있는 방법이 없다.\n",
    "    - 데이터셋을 훈련 세트와 검증 세트로 나눈다.\n",
    "    - 모델을 훈련 세트에서 훈련시키고 검증 세트에서 평가한다.\n",
    "    - 검증 세트는 모델이 본 적이 없는 데이터여야 한다.\n",
    "    - 검증 세트에서 모델의 성능이 좋지 않다면 모델이 훈련 데이터에 과적합(overfitting)되었음을 나타낸다.\n",
    "    - 과적합은 모델이 훈련 데이터의 노이즈나 세부 사항을 학습하여 새로운 데이터에 일반화하는 능력이 떨어지는 현상이다.\n",
    "\n",
    "5. 카운트 기반 언어 모델은 적절한 수준의 토큰 생성을 즉각적으로 숭행하므뢰 자원완성 시스템에 적합하다.\n",
    "    - n-그램 크기가 일반적으로 작기 때문에(최대 n=5) 단어로 토큰화된 말뭉치에 잘 맞는다.\n",
    "    - 이른 넘어가면 너무 많은 매모리가 소모되고 처리속도가 느려진다.\n",
    "\n",
    "6. 단어 수준 토큰화는 또 다른 단점이 있다.\n",
    "    - 카운터 기반 모델은 OOV(Out-Of-Vocabulary) 단어를 처리하지 못한다.\n",
    "    - \"according to WHO, COVID-19 is a\"인 경우, \"COVNID-19\"가 훈련 데이터에 없다면 모델이 \"is a\"에 도달하기까지 반복적으로 백오프할 것이다.\n",
    "    \n",
    "7. 카운트 기반 모델은 언어에서 멀리 떨어진 의존성을 포착할 수 없다.\n",
    "    - 예를 들어, \"The book that the professor who won the award recommended was fascinating.\" 문장에서 \"book\"과 \"was\" 사이에는 여러 단어가 있다.\n",
    "    - 카운트 기반 모델은 이런 긴 거리 의존성을 포착하는 데 어려움이 있다.\n",
    "    - 반면 신경망 언어 모델은 문맥 전체를 고려하여 더 긴 거리 의존성을 포착할 수 있다.\n",
    "    - 문맥 1000개의 카운트 기반 모델은 n=1에서 n=1000까지 모든 n-그램을 위한 카운트를 저장해야 하므로 엄청난 메모리가 필요하다.\n",
    "\n",
    "8. 훈련 후에 n-그램 카운트가 고정되므로 후속 작업에 적용할 수 없다.\n",
    "    - 새로운 단어가 나타나면 어휘사전을 업데이트하고 모델을 재훈련해야 한다.\n",
    "    - 반면 신경망 언어 모델은 새로운 단어에 대한 임베딩을 학습하여 새로운 단어를 처리할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6e36f88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountLanguageModel:\n",
    "    \"\"\"\n",
    "    카운트 기반 확률 추정을 사용하는 n-그램 언어 모델을 구현합니다.\n",
    "    n-그램까지 가변적인 문맥 길이를 지원합니다.\n",
    "    \"\"\"\n",
    "    def __init__(self, n):\n",
    "        \"\"\"\n",
    "        최대 n-그램 길이로 모델을 초기화합니다.\n",
    "        Args:\n",
    "            n (int): 사용할 n-그램의 최대 길이\n",
    "        \"\"\"\n",
    "        self.n = n # 최대 n-그램 길이\n",
    "        # 각 n-그램 길이에 대한 딕셔너리 목록\n",
    "        #   - n=3이고 말 뭉치가 \"Language models are powerful. Language models are useful.\"인 경우\n",
    "        #   - 소문자로 변환 후 구두점을 제거하면 다음과 같은 값이 들어간다.\n",
    "        #   - self.ngram_counts[0]: {(): {'language': 2, 'models': 2, 'are': 2, 'powerful': 1, 'useful': 1}}\n",
    "        #   - self.ngram_counts[1]: {('language',): {'models': 2}, ('models',): {'are': 2}, ('are',): {'powerful': 1, 'useful': 1}, ('powerful',): {'language': 1}}\n",
    "        #   - self.ngram_counts[2]: {('language', 'models'): {'are': 2}, ('models', 'are'): {'powerful': 1, 'useful': 1}, ('are', 'powerful'): {'language': 1}, ('powerful', 'language'): {'models': 1}}\n",
    "        self.ngram_counts = [{} for _ in range(n)]\n",
    "        self.total_unigrams = 0\n",
    "        \n",
    "    def predict_next_token(self, context):\n",
    "        \"\"\"\n",
    "        주어진 문맥에서 가장 가능성 있는 다음 토큰을 예측합니다.\n",
    "        백오프 전략 사용: 가장 큰 n-그램부터 시도한 후 더 작은 n-그램으로 백오프합니다.\n",
    "        Args:\n",
    "            context (list): 예측을 위한 문맥을 제공하는 토큰 목록\n",
    "        Returns:\n",
    "            str: 가장 가능성 있는 다음 토큰, 예측을 할 수 없는 경우 None\n",
    "        \"\"\"\n",
    "        # context=[\"language\", \"models\", \"are\"]이고 n=3인 경우\n",
    "        # 첫 번재 반복: context_n = (\"models\", \"are\") -> 2-그램\n",
    "        # ('models', 'are'): {'powerful': 1, 'useful': 1} -> powerful, useful\n",
    "        \n",
    "        # context==[\"language\", \"models\"]이고 n=3인 경우\n",
    "        # 첫 번재 반복: context_n = (\"language\", \"models\") -> 2\n",
    "        # ('language', 'models'): {'are': 2} -> are\n",
    "        \n",
    "        # context==[\"english\", \"language\"]이고 n=3인 경우\n",
    "        # 첫 번재 반복: context_n = (\"english\", \"language\") -> ngram_counts[2]에서 못 찾음\n",
    "        # ngram_counts[1]로 백오프 하여 두 번째 반복: context_n = (\"language\",) \n",
    "        # ('language',): {'models': 2} -> models\n",
    "         \n",
    "        for n in range(self.n, 1, -1): # 가장 큰 n-그램으로 시작하여 더 작은 것으로 백오프\n",
    "            if len(context) >= n - 1: # 충분한 문맥이 있는지 확인\n",
    "                context_n = tuple(context[-(n-1):]) # 이 n-그램에 대한 관련 문맥 가져오기\n",
    "                counts = self.ngram_counts[n-1].get(context_n) # 해당 문맥에 대한 다음 토큰 카운트 가져오기\n",
    "                if counts: # 문맥이 존재하면 가장 빈도가 높은 토큰 반환\n",
    "                    # counts 딕셔너리에서 가장 큰 값에 해당하는 키를 찾는 파이썬 코드입니다. \n",
    "                    #    - counts.items()는 (키, 값) 쌍으로 이루어진 튜플들을 반환하고\n",
    "                    #    - lambda x: x[1] 튜플의 두 번째 요소인 값(value)을 기준으로 정렬하겠다는 의미.\n",
    "                    #       - x는 각 튜플을 나타내며, x[1]은 그 튜플의 두 번째 요소인 값을 의미합니다.\n",
    "                    #    - max() 함수는 이 기준에 따라 가장 큰 값을 가진 튜플을 찾고, [0]은 해당 튜플에서 첫 번째 요소인 키를 추출합니다\n",
    "                    return max(counts.items(), key=lambda x: x[1])[0]\n",
    "                \n",
    "        # 더 큰 문맥이 일치하지 않는 경우 유니그램으로 백오프        \n",
    "        unigram_counts = self.ngram_counts[0].get(())\n",
    "        if unigram_counts: # 유니그램이 존재하면 가장 빈도가 높은 토큰 반환\n",
    "            return max(unigram_counts.items(), key=lambda x: x[1])[0]\n",
    "        return None\n",
    "\n",
    "    # 확률 계산 메서드\n",
    "    def get_probability(self, token, context):\n",
    "        for n in range(self.n, 1, -1):\n",
    "            if len(context) >= n - 1:\n",
    "                context_n = tuple(context[-(n - 1):])\n",
    "                counts = self.ngram_counts[n - 1].get(context_n)\n",
    "                if counts:\n",
    "                    total = sum(counts.values())\n",
    "                    count = counts.get(token, 0)\n",
    "                    if count > 0:\n",
    "                        return count / total\n",
    "        unigram_counts = self.ngram_counts[0].get(())\n",
    "        count = unigram_counts.get(token, 0)\n",
    "        V = len(unigram_counts)\n",
    "        return (count + 1) / (self.total_unigrams + V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a45aaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, tokens):\n",
    "    \"\"\"\n",
    "    학습 데이터에서 n-그램을 계수하여 언어 모델을 학습시킵니다.\n",
    "    Args:\n",
    "        model (CountLanguageModel): 학습할 모델\n",
    "        tokens (list): 학습 말뭉치의 토큰 목록\n",
    "    \"\"\"\n",
    "    model.total_unigrams += len(tokens)\n",
    "    \n",
    "    # 1부터 n까지 각 n-그램 크기에 대한 모델 학습\n",
    "    for n in range(1, model.n + 1):\n",
    "        # n-그램 카운트 딕셔너리 가져오기\n",
    "        counts = model.ngram_counts[n-1]\n",
    "        # 말뭉치 위에 크기 n의 창을 슬라이드\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            # 문맥(n-1 토큰)과 다음 토큰으로 분할\n",
    "            #   - 문맥과 뒤따르는 후속 토큰을 추출하여 문맥과 딕셔너리를 매칭한 중첩된 딕셔너리를 만든다.\n",
    "            #   - 예: n=3이고 tokens=[\"language\", \"models\", \"are\", \"powerful\"]인 경우\n",
    "            #       - i=0: context = (\"language\", \"models\"), next_token = \"are\"\n",
    "            #       - i=1: context = (\"models\", \"are\"), next_token = \"powerful\"\n",
    "            context = tuple(tokens[i:i + n -1])\n",
    "            next_token = tokens[i + n - 1]\n",
    "            # 필요한 경우 이 문맥에 대한 카운트 딕셔너리 초기화\n",
    "            if context not in counts:\n",
    "                counts[context] = defaultdict(int)\n",
    "            # 이 문맥-토큰 쌍의 카운트 증가\n",
    "            counts[context][next_token] = counts[context][next_token] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fce6c674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리 임포트\n",
    "import re         # 정규 표현식을 위한 라이브러리 (텍스트 토큰화)\n",
    "import requests   # 말뭉치 다운로드를 위한 라이브러리\n",
    "import gzip       # 다운로드한 말뭉치 압축 해제를 위한 라이브러리\n",
    "import io         # 바이트 스트림 처리를 위한 라이브러리\n",
    "import math       # 수학 연산을 위한 라이브러리 (로그, 지수)\n",
    "import random     # 난수 생성을 위한 라이브러리\n",
    "from collections import defaultdict  # 효율적인 딕셔너리 연산을 위한 라이브러리\n",
    "import pickle, os # 모델 저장 및 로드를 위한 라이브러리\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    텍스트를 단어와 마침표로 토큰화합니다.\n",
    "    Args:\n",
    "        text (str): 토큰화할 입력 텍스트\n",
    "    Returns:\n",
    "        list: 단어나 마침표와 일치하는 소문자 토큰 목록\n",
    "    \"\"\"\n",
    "    return re.findall(r\"\\b[a-zA-Z0-9]+\\b|[.]\", text.lower())\n",
    "\n",
    "def download_corpus(url):\n",
    "    \"\"\"\n",
    "    주어진 URL에서 gzip으로 압축된 말뭉치 파일을 다운로드하고 압축을 해제합니다.\n",
    "    Args:\n",
    "        url (str): gzip으로 압축된 말뭉치 파일의 URL\n",
    "    Returns:\n",
    "        str: 말뭉치의 디코딩된 텍스트 내용\n",
    "    Raises:\n",
    "        HTTPError: 다운로드 실패 시 발생하는 예외\n",
    "    \"\"\"\n",
    "    print(f\"{url}에서 말뭉치 다운로드 중...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # 잘못된 HTTP 응답에 대해 예외를 발생시킴\n",
    "    print(\"말뭉치 압축 해제 및 읽는 중...\")\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(response.content)) as f:\n",
    "        corpus = f.read().decode('utf-8')\n",
    "    print(f\"말뭉치 크기: {len(corpus)} 문자\")\n",
    "    return corpus\n",
    "\n",
    "def download_and_prepare_data(data_url):\n",
    "    \"\"\"\n",
    "    훈련 및 테스트 데이터를 다운로드하고 준비합니다.\n",
    "    Args:\n",
    "        data_url (str): 다운로드할 말뭉치의 URL\n",
    "    Returns:\n",
    "        tuple: (훈련_토큰, 테스트_토큰) 90/10으로 분할\n",
    "    \"\"\"\n",
    "    # 말뭉치 다운로드 및 추출\n",
    "    corpus = download_corpus(data_url)\n",
    "    # 텍스트를 토큰으로 변환\n",
    "    tokens = tokenize(corpus)\n",
    "    # 훈련(90%) 및 테스트(10%) 세트로 분할\n",
    "    split_index = int(len(tokens) * 0.9)\n",
    "    train_corpus = tokens[:split_index]\n",
    "    test_corpus = tokens[split_index:]\n",
    "    return train_corpus, test_corpus\n",
    "\n",
    "def compute_perplexity(model, tokens, context_size):\n",
    "    \"\"\"\n",
    "    주어진 토큰에 대한 모델의 혼잡도를 계산합니다.\n",
    "    Args:\n",
    "        model (CountLanguageModel): 학습된 언어 모델\n",
    "        tokens (list): 평가할 토큰 목록\n",
    "        context_size (int): 고려할 최대 문맥 크기\n",
    "    Returns:\n",
    "        float: 혼잡도 점수 (낮을수록 좋음)\n",
    "    \"\"\"\n",
    "    # 빈 토큰 목록 처리\n",
    "    if not tokens:\n",
    "        return float('inf')\n",
    "    # 로그 우도 누적 변수 초기화\n",
    "    total_log_likelihood = 0\n",
    "    num_tokens = len(tokens)\n",
    "    # 각 토큰의 문맥에 주어진 확률 계산\n",
    "    for i in range(num_tokens):\n",
    "        # 적절한 문맥 윈도 가져오기, 시퀀스 시작 처리\n",
    "        context_start = max(0, i - context_size)\n",
    "        context = tuple(tokens[context_start:i])\n",
    "        token = tokens[i]\n",
    "        # 주어진 문맥에 대한 토큰의 확률 가져오기\n",
    "        probability = model.get_probability(token, context)\n",
    "        # 로그 확률 누적 (수치적 안정성을 위해 로그 사용)\n",
    "        total_log_likelihood += math.log(probability)\n",
    "    # 평균 로그 우도 계산\n",
    "    average_log_likelihood = total_log_likelihood / num_tokens\n",
    "    # 혼잡도로 변환: exp(-평균 로그 우도)\n",
    "    # 낮은 혼잡도는 더 나은 모델 성능을 나타냄\n",
    "    perplexity = math.exp(-average_log_likelihood)\n",
    "    return perplexity\n",
    "\n",
    "def generate_text(model, context, num_tokens):\n",
    "    \"\"\"\n",
    "    모델에서 반복적으로 샘플링하여 텍스트를 생성합니다.\n",
    "    Args:\n",
    "        model (CountLanguageModel): 학습된 언어 모델\n",
    "        context (list): 초기 문맥 토큰\n",
    "        num_tokens (int): 생성할 토큰 수\n",
    "    Returns:\n",
    "        str: 초기 문맥을 포함한 생성된 텍스트\n",
    "    \"\"\"\n",
    "    # 제공된 문맥으로 시작\n",
    "    generated = list(context)\n",
    "    # 원하는 길이에 도달할 때까지 새 토큰 생성\n",
    "    while len(generated) - len(context) < num_tokens:\n",
    "        # 예측을 위한 문맥으로 마지막 n-1 토큰 사용\n",
    "        next_token = model.predict_next_token(generated[-(model.n-1):])\n",
    "        generated.append(next_token)\n",
    "        # 충분한 토큰을 생성하고 마침표를 찾으면 중지\n",
    "        # 이는 완전한 문장을 보장하는 데 도움이 됨\n",
    "        if len(generated) - len(context) >= num_tokens and next_token == '.':\n",
    "            break\n",
    "    # 읽을 수 있는 텍스트를 만들기 위해 토큰을 공백으로 연결\n",
    "    return ' '.join(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae05bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.thelmbook.com/data/brown에서 말뭉치 다운로드 중...\n",
      "말뭉치 압축 해제 및 읽는 중...\n",
      "말뭉치 크기: 6185606 문자\n",
      "훈련 토큰 수: 975228\n",
      "테스트 토큰 수: 108359\n",
      "모델 훈련 완료.\n"
     ]
    }
   ],
   "source": [
    "import random   \n",
    "\n",
    "random.seed(42)\n",
    "n = 5\n",
    "\n",
    "# Brown 말뭉치를 다운로드하여 준비합니다.\n",
    "#   - 1961년에 출판된 미국 영어 텍스트에서 추출한 백만 개의 단어집합\n",
    "data_url = \"https://www.thelmbook.com/data/brown\"\n",
    "train_corpus, test_corpus = download_and_prepare_data(data_url)\n",
    "print(f\"훈련 토큰 수: {len(train_corpus)}\")\n",
    "print(f\"테스트 토큰 수: {len(test_corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c42ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 훈련 완료.\n"
     ]
    }
   ],
   "source": [
    "model = CountLanguageModel(n=n)\n",
    "train(model, train_corpus)\n",
    "print(\"모델 훈련 완료.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "14e1e8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 세트 혼잡도: 299.06\n"
     ]
    }
   ],
   "source": [
    "perplexity = compute_perplexity(model, test_corpus, context_size=n-1)\n",
    "print(f\"테스트 세트 혼잡도: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42e44c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문맥: i will build a\n",
      "다음 토큰: wall\n",
      "생성된 텍스트: i will build a wall to keep the people in and added so long\n",
      "\n",
      "문맥: the best place to\n",
      "다음 토큰: live\n",
      "생성된 텍스트: the best place to live in 30 per cent to get happiness for yourself\n",
      "\n",
      "문맥: she was riding a\n",
      "다음 토큰: horse\n",
      "생성된 텍스트: she was riding a horse and showing a dog are very similar your aids\n"
     ]
    }
   ],
   "source": [
    "# 샘플 문맥으로 모델을 테스트합니다.\n",
    "contexts = [\n",
    "    \"i will build a\",\n",
    "    \"the best place to\",\n",
    "    \"she was riding a\"\n",
    "]\n",
    "\n",
    "# 각 문맥에 대해 완성을 생성합니다.\n",
    "for context in contexts:\n",
    "    tokens = tokenize(context)\n",
    "    next_token = model.predict_next_token(tokens)\n",
    "    print(f\"\\n문맥: {context}\")\n",
    "    print(f\"다음 토큰: {next_token}\")\n",
    "    print(f\"생성된 텍스트: {generate_text(model, tokens, 10)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
