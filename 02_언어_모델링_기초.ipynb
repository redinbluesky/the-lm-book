{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3461b873",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/the-lm-book/blob/main/02_언어_모델링_기초.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ffb53f",
   "metadata": {},
   "source": [
    "# 목차\n",
    "* [Chapter 2_1 BoW](#chapter2_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fa90e9",
   "metadata": {},
   "source": [
    "## Chapter 2_1 BoW <a class=\"anchor\" id=\"chapter2_1\"></a>\n",
    "1. 문서 집합이 있고 각 문서의 주요 토픽을 예측하고 싶다고 가정한다.\n",
    "   - 토픽이 사전에 정의가 되어 있다면 이 작업은 분류 작업이 된다.\n",
    "      - 두 개의 토픽이 있다면 이진 분류, 세 개 이상의 토픽이 있다면 다중 분류가 된다.\n",
    "   \n",
    "2. 다중 분류에서 데이터 셋은 다음과 같이 표시할 수 있다.\n",
    "   - {(xᵢ, yᵢ)}ᵢ₌₁ⁿ 쌍으로 구성된다.\n",
    "      - xᵢ: 문서를 나타태는 텍스트\n",
    "      - n: 샘플의 개수\n",
    "      - yᵢ: 문서의 토픽을 나타내는 정수\n",
    "         - yᵢ ∈ {1, 2, ..., C} C는 가능한 토픽의 개수\n",
    "         - 예를 들어 1은 \"music\", 2는 \"sports\", 3은 \"politics\"를 나타낼 수 있다.\n",
    "\n",
    "3. 기계는 사람처럼 텍스트를 처리하지 못한다.\n",
    "   - 텍스트를 숫자 벡터로 변환해야 한다.\n",
    "   \n",
    "4. Bag-of-Words(BoW) 모델은 텍스트를 숫자 벡터로 변환하는 간단한 방법이다.\n",
    "   - 10개의 문서를 예를들어 작동 방식을 설명한다.\n",
    "     - 문서 텍스트\n",
    "         1. Movies are fun for everyone.\n",
    "         2. Watching movies is great fun.\n",
    "         3. Enjoy a great movie today.\n",
    "         4. Research is interesting and important.\n",
    "         5. Learning math is very important.\n",
    "         6. Science discovery is interesting.\n",
    "         7. Rock is great to listen to.\n",
    "         8. Listen to music for fun.\n",
    "         9. Music is fun for everyone.\n",
    "         10. Listen to folk music! \n",
    "\n",
    "5. 머신러닝에서 사용되는 텍스트 문서 집합을 말뭉치(corpus)라고 하며 BoW 방법을 말뭉치에 적용하려면 두 단계를 거친다.\n",
    "   - 어휘사전을 만든다\n",
    "      - 말뭉치에 등장하는 고유 단어를 나열해 어휘사전(vocabulary)을 만든다.\n",
    "   - 문서를 벡터로 변환한다.\n",
    "      - 각각의 문서를 특성 벡터로 변환한다.\n",
    "      - 이 벡터의 각 차원은 어휘사전에 있는 한 단어를 나타낸다.\n",
    "      - 벡터에는 단어 있음/없음, 문서에서의 등장빈도가 입력된다.\n",
    "\n",
    "6. 10개의 문서로 구성된 말뭉치를 사용해 모든 고유한 단어를 알파펫 순서로 나열해 어휘사전을 만든다.\n",
    "   - 구두점을 삭제하고, 단어를 소문자로 변경하고, 중복 단어는 삭제한다.\n",
    "   - vocabulary = [\"a\", \"and\", \"are\", \"discovery\", \"enjoy\", \"everyone\", \"folk\", \"for\", \"fun\", \"great\", \"important\", \"interesting\", \"is\", \"listen\", \"learning\", \"math\", \"movie\", \"movies\", \"music\", \"of\", \"rock\", \"science\", \"to\", \"very\", \"watching\"]\n",
    "\n",
    "7. 문서를 더 나눌 수 없는 작은 부분으로 분활하는 것을 토큰화(tokenization)이라고한다.\n",
    "   - 분할된 각 부분을 토큰(token)이라고 한다.\n",
    "\n",
    "8. 때때로 단어를 더 작은 단위인 부분단어(subword)로 분할하여 어휘사전을 적절한 크기로 유지하는 게 유용할 때가 있습니다.\n",
    "   - 예를 들어 \"unhappiness\"라는 단어를 \"un\", \"happi\", \"ness\"라는 세 개의 부분단어로 분할할 수 있다.\n",
    "   - 부분단어 토큰화는 특히 희귀 단어와 신조어에 유용하다.\n",
    "   - 날리 사용되는 부분단어 토큰화 알고리즘으로는 Byte Pair Encoding(BPE)과 WordPiece가 있다.\n",
    "   - do, does, doing, did와 같이 모든 영어 단어의 표면형태(surface form)는 수백만 개가 될 수 있고, 더 복잡한 형태를 가진 언어는 그 개수가 훨씬 더 많다.\n",
    "   - 모든 단어의 표면형태를 어휘사전에 포함시키는 것은 비현실적이다.\n",
    "   - 부분단어 토큰화를 사용하면 어휘사전의 크기를 줄이고, 모델이 훈련 데이터에 나타나지 않는 단어를 처리할 수 있게 한다\n",
    "\n",
    "9. 단어는 토큰의 한 형태로 문서에서 더 나눌 수 없는 단위를 의미하며, 토큰과 단어는 혼용되어 사용되는 경우가 많다.\n",
    "   - BoW는 단어와 부분단어 모두를 다룰 수 있지만, 원래 단어를 위해 설계되었다.\n",
    "\n",
    "10. 특성 벡터로 문서 단어 행렬(Document-Term Matrix, DTM)을 만들 수 있다.\n",
    "   - 행은 문서, 열은 어휘사전의 단어(토큰)를 나타낸다.\n",
    "   - 각 셀에는 해당 단어가 문서에 등장한 횟수가 들어간다.\n",
    "   - 아래의 이미지에서 DTM의 각 행은 문서를 나타내고, 각 열은 어휘사전의 단어를 나타낸다.\n",
    "   - 만약 단어가 문서에 등장하지 않으면 해당 셀에는 0이 들어가고, 등장하면 등장 횟수가 들어간다.\n",
    "\n",
    "      ![Document-Term Matrix](image/02-01-doc-term-matrix.png)\n",
    "\n",
    "11. 문서 2 \"Watching movies is great fun.\"의 특성 벡터는 다음과 같다.\n",
    "   - [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
    "   - 이 벡터에서 세 번째 차원(단어 \"are\")은 0이다. 이는 \"are\"가 문서에 등장하지 않음을 의미한다.\n",
    "   - 아홉 번째 차원(단어 \"fun\")은 1이다. 이는 \"fun\"이 문서에 한 번 등장했음을 의미한다.\n",
    "\n",
    "12. 자연어에서 단어의 빈도는 지프의 법칙(Zipf's Law)을 따른다.\n",
    "   - 단어를 빈도순으로 나열했들 때 단어의 빈도가 순서에 반비례한다는 것이다.\n",
    "   - 즉, 가장 빈도가 높은 단어는 두 번째로 빈도가 높은 단어의 두 배, 세 번째로 빈도가 높은 단어의 세 배가 된다.\n",
    "   - 이러한 현상은 대부분의 언어에서 관찰된다.\n",
    "   - 예를 들어 영어에서 \"the\", \"is\", \"in\"과 같은 단어는 매우 자주 등장하는 반면, \"quixotic\", \"serendipity\"와 같은 단어는 매우 드물게 등장한다.\n",
    "   - 지프의 법칙은 단어 빈도가 매우 불균등하게 분포되어 있음을 의미한다.\n",
    "   - 이는 BoW 표현에서 많은 단어가 대부분의 문서에서 등장하지 않음을 의미한다.\n",
    "   - 결과적으로 DTM은 매우 희소(sparse)해진다.\n",
    "   - 희소 행렬은 대부분의 요소가 0인 행렬을 의미한다.\n",
    "   - 희소 행렬은 메모리 효율성과 계산 효율성 측면에서 도전 과제를 제기할 수 있다.\n",
    "\n",
    "13. 이런 특성 벡터를 사용해 문서의 토픽을 예측하도록 신경망을 훈련시킬 수 있다.\n",
    "   - 문서에 레이블을 할당하는 레이블링을 수행한다.\n",
    "      - 레이블링은 수작업이나 알로리즘을 통해 수행할 수 있다.\n",
    "      - 알고리즘을 사용하는 경우 정확성을 확보하기 위해 사람이 검증해야 하는 경우가 많다.\n",
    "      - 문서를 직접 잀고 세 개의 토픽 중 가장 적합한 것을 골라 레이블을 할당한다. \n",
    "   - 이미지로 표현하면 아래와 같다.\n",
    "   \n",
    "      ![Document Labeling](image/02-01-document-labeling.png)\n",
    "    \n",
    "\n",
    "14. 고급 채팅 언어 모델(chat language model, ChatLM)은 전문가 모델의 패널(panel)을 활용해 자동화된 문서 레이블링을 매우 정확하게 수행할 수 있다.\n",
    "   - 세 개의 LLM을 사용하는 경우 두 개 이상의 모델이 문서에 동일하게 할당한 레이블을 채택한다.\n",
    "   - 세 개의 LLM이 모두 다른 레이블을 예측하면 사람이 결정하거나 네 번째 모델을 사용해 교착 상태를 해결할 수 있다.\n",
    "   - 많은 비즈니스 문제에서 LLM이 빠르고 더 안정적인 레이블링을 제공하므로 수동 레이블링은 점점 사라지고 있다.\n",
    "\n",
    "15. 이진 분류기는 일반적으로 시그모이드 활성화 함수와 이진 크로스 엔트로피 손실을 사용한다.\n",
    "   - 세 개 이상의 클래스를 다루는 작업에서는 일반적으로 소프트맥스 활성화 함수와 크로스 엔트로피 손실을 사용한다.\n",
    "   - 소프트맥스 함수는 다음과 같이 정의된다.\n",
    "      - softmax(zᵢ, k) = eᶻₖ / Σⱼ₌₁ᴰeᶻⱼ\n",
    "         - zᵢ: D차원의 로짓 벡터, 로짓은 활성화 함수를 적용하기 전의 신경망 출력값\n",
    "         - k: 소프트맥스가 계산되는 인덱스\n",
    "         - e: 자연상수(오일러수 약 2.718)\n",
    "         - Σⱼ: 모든 클래스 j에 대한 합계\n",
    "\n",
    "16. 아래 그림은 한 신경망의 출력 층 o를 나타낸다.\n",
    "   - 로짓 zₒ는 밝은 녹샛 상자 안의 값으로 활성화 함수가 적용되기 전의 신경망 출력값이다.\n",
    "   - Zₒ= [zₒ₁, zₒ₂, zₒ₃]ᵀ\n",
    "   \n",
    "      ![Softmax Function](image/02-01-softmax-function.png)\n",
    "\n",
    "   - 예를 들어 유닛 o, 2에 대한 소프트맥스는 다음과 같이 계산된다.\n",
    "      - softmax(zₒ₂, 2) = eᶻₒ₂ / (eᶻₒ₁ + eᶻₒ₂ + eᶻₒ₃)  \n",
    "   - 소프트맥스 함수는 벡터를 이산 확률 분포(discrete probability distribution)로 변환한다.\n",
    "      - 유한집합의 값에 확률을 할당하여 그 합이 1이되되록 한다.\n",
    "         - 유한집합이란 셀수 있는 개수의 원소가 포함된 집합이다.\n",
    "         - 클래스 1, 클래스 2, 클래스 3의 세 개의 클래스가 있는 경우 유한집합은 {1, 2, 3}이 된다.\n",
    "         - 소프트맥스 함수는 각 클래스를 확률로 매핑하며, 모든 클래스의 확률 합은 1이 된다.\n",
    "      - Σⱼ₌₁ᴰsoftmax(zᵢ, j) = 1\n",
    "\n",
    "17. \"cinema\", \"music\", \"science\" 세 개의 로짓이 z = [2.0, 1.0, 0.5]라고 가정한다.\n",
    "   - 각 로직에 대한 오일러 지수를 계산한다.\n",
    "      - ez¹ = e².⁰ ≈ 7.389\n",
    "      - ez² = e¹.⁰ ≈ 2.718\n",
    "      - ez³ = e⁰.⁵ ≈ 1.649 \n",
    "   - 모든 값을 더한다.\n",
    "      - Σ = 7.389 + 2.718 + 1.649 ≈ 11.756\n",
    "   - 소프트맥스 함수 공식을 사용해 확률을 계산한다.\n",
    "      - softmax(z, 1) = ez¹ / Σ ≈ 7.389 / 11.756 ≈ 0.628\n",
    "      - softmax(z, 2) = ez² / Σ ≈ 2.718 / 11.756 ≈ 0.231\n",
    "      - softmax(z, 3) = ez³ / Σ ≈ 1.649 / 11.756 ≈ 0.140\n",
    "   - 따라서 클래스 \"cinema\"의 확률은 약 62.8%, \"music\"의 확률은 약 23.1%, \"science\"의 확률은 약 14.0%이다.\n",
    "   - 세 확률의 합은 1이 된다.\n",
    "      - 0.628 + 0.231 + 0.140 ≈ 0.999 (반올림으로 인해 약간의 오차가 발생할 수 있음)\n",
    "\n",
    "18. 신경망의 소프트맥스 출력은 모두 더하면 1이고 클래스 가능도와 닮았지만, 진정한 통계적 확률보다는 '확률 점수'로 이해하는 것이 낫다.\n",
    "   - 신경망이 출력하는 값은 모델이 각 클래스에 대해 얼마나 확신하는지를 나타내는 점수이다.\n",
    "   - 이 점수는 실제 확률과 다를 수 있다.\n",
    "   - 예를 들어, 모델이 클래스 A에 대해 0.9의 확률을 출력했더라도, 실제로는 클래스 A가 정답일 확률이 0.7일 수 있다.\n",
    "   - 따라서 소프트맥스 출력은 모델의 신뢰도를 나타내는 지표로 사용해야 한다.\n",
    "\n",
    "19. 크로스 엔트로피 손실은 예측 확률이 정답 분포와 얼마나 잘 맞는지를 측정한다.\n",
    "   - 정답 분포는 일반적으로 원삿 벡터(one-hot vector)로 표현된다.\n",
    "      - 원삿 벡터는 정답 클래스에 1을 할당하고 나머지 클래스에는 0을 할당하는 벡터이다.\n",
    "   - 클래스 1 = [1, 0, 0], 클래스 2 = [0, 1, 0], 클래스 3 = [0, 0, 1]로 표현된다.\n",
    "   - 하나의 크로스 엔트로피 손실은 다음과 같이 계산한다.\n",
    "      - loss(ỹ, y) = - Σⱼ₌₁ᴰ yⱼ log(ỹⱼ)\n",
    "         - ỹ: 신경망의 소프트맥스 출력 확률 벡터\n",
    "         - y: 정답 원삿 벡터\n",
    "         - D: 클래스의 개수\n",
    "         - log: 자연로그\n",
    "         - yⱼ: 정답 원삿 벡터의 j번째 요소\n",
    "         - ỹⱼ: 신경망의 소프트맥스 출력 확률의 j번째 요소\n",
    "         - y가 원핫 인코딩되어 있으므로 정답 클래스에 해당하는 원소만 덧셈 계산에 포함된다.\n",
    "            - 하나의 원소만 남겨 이 덧셈을 단순화할 수 있다.\n",
    "      - 정답 클래스가 c라고 가정하면 yc = 1이고 k=!c인 모든 경우에 yk = 0이 된다.\n",
    "         - 따라서 크로스 엔트로피 손실은 다음과 같이 단순화된다.\n",
    "            - loss(ỹ, y) = - log(ỹc)\n",
    "         - 간소화된 이 식은 손실이 정답 클래스에 할당된 확률의 음의 로그에 해당하는 것을 보여준다.\n",
    "      - N개의 샘플이 있는 경우 편균 손실은 다음과 같이 계산된다.\n",
    "         - Loss = (1/N) Σᵢ₌₁ᴺ log(ỹcᵢ)\n",
    "            - cᵢ: i번재 샘플에 대한 정답 클래스 인덱스\n",
    "\n",
    "20. 출력 층에 소프트맥스 함수를 사용할 때 크로스 엔트로피 손실은 신경망이 정답 클래스에는 높은 확률을 할당하고, 다른 클래스에는 낮은 확률을 할당하도록 유도한다.\n",
    "   - \"cinema\", \"music\", \"science\" 세 개의 클래스를 가진 문서 분류 예제의 경우 신경망의 세 개의 로짓을 생성한다.\n",
    "   - 이 로짓이 소프트맥스 함수를 통과하여 각 클래스에 대한 확률로 변환된다.\n",
    "   - 이 점수와 원핫 인코딩된 정답 레이블로 크로스 엔트로피 손실이 계산된다.\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86c6414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, torch, torch.nn as nn\n",
    "\n",
    "# 랜덤 시드를 설정하여 일정한 결과를 얻도록 함\n",
    "#   - 재현성 보장\n",
    "#      - 성능에 변화가 있다면 난수가 아니라 코드나 하이퍼 파라미터 때문임을 알 수 있음\n",
    "#      - 팀워크에서 중요하며, 동료들이 동일한 조건에서 문제를 조하사할 수 있게 한다.\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 문서 데이터\n",
    "docs = [\n",
    "\"Movies are fun for everyone.\",\n",
    "\"Watching movies is great fun.\",\n",
    "\"Enjoy a great movie today.\",\n",
    "\"Research is interesting and important.\",\n",
    "\"Learning math is very important.\",\n",
    "\"Science discovery is interesting.\",\n",
    "\"Rock is great to listen to.\",\n",
    "\"Listen to music for fun.\",\n",
    "\"Music is fun for everyone.\",\n",
    "\"Listen to folk music!\"\n",
    "]\n",
    "\n",
    "# 레이블 데이터\n",
    "labels = [1, 1, 1, 3, 3, 3, 2, 2, 2, 2]  # 1: cinema, 2: music, 3: science\n",
    "\n",
    "# 클래스 수 계산\n",
    "num_class = len(set(labels))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f86d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 단어를 소문자 화하고 단어로 분할\n",
    "def tokenize(text):\n",
    "    # \\w+ : 단어 문자를 하나 이상 매칭 (알파벳, 숫자, 밑줄)\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "# 어휘사전을 만든다.\n",
    "def get_vocabulary(texts):\n",
    "    # 문서와 정규식 표현으로 추출한 단어를 순회하면서 말뭉치를 토큰집합으로 변환한다.\n",
    "    tokens = {token for text in texts for token in tokenize(text)}\n",
    "    \n",
    "    # 각 토큰을 알파벳 순서대로 정렬하고 인덱스를 부여하여 딕셔너리로 반환한다.\n",
    "    return {word: idx for idx, word in enumerate(sorted(tokens))}\n",
    "\n",
    "vocabulary = get_vocabulary(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9ab9156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서를 특성 벡터로 변화하는 특성 추출 함수\n",
    "def doc_to_bow(doc, vocabulary):\n",
    "    # 문서를 토큰화한다.\n",
    "    tokens = set(tokenize(doc))\n",
    "    \n",
    "    # 단어 집합 크기만큼 0으로 초기화된 벡터 생성\n",
    "    bow = [0] * len(vocabulary)  \n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in vocabulary:\n",
    "            bow[vocabulary[token]] = 1  # 단어가 존재하면 해당 인덱스를 1로 설정\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4135cd50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 26])\n",
      "tensor([0, 0, 0, 2, 2, 2, 1, 1, 1, 1])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# (10, 26) 크기의 특성 행렬 생성\n",
    "#   - 10개의 문서, 26개의 어휘사전 토큰 크기\n",
    "vectors = torch.tensor([doc_to_bow(doc, vocabulary) for doc in docs], dtype=torch.float)\n",
    "print(vectors.shape)  # 특성 행렬의 크기 출력\n",
    "\n",
    "# 원핫 인코딩 벡터가 아니라 정수 인덱스 레이블 벡터 생성\n",
    "#   - 파이토치의 크로스 엔트로피 손실 함수가 정수 레이블을 요구하기 때문\n",
    "#   - \"-1\" 빼는 이유는 파이토치 모델과 크로스엔트로피 함수가 0부터 시작하는 레이블을 요구하기 때문\n",
    "labels = torch.tensor(labels, dtype=torch.long)-1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02eb6621",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(vocabulary)  # 입력 차원 (어휘사전 크기)\n",
    "hidden_dim = 50              # 은닉층 차원\n",
    "output_dim = num_class       # 출력 차원 (클래스 수)\n",
    "\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(SimpleClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  # 첫 번째 완전 연결 층\n",
    "        self.relu = nn.ReLU()                        # ReLU 활성화 함수\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim) # 두 번째 완전 연결 층\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 두 개의 층으로 구성된 피드포워드 신경망 구성\n",
    "        out = self.fc1(x)      # 입력을 첫 번째 층에 통과\n",
    "        out = self.relu(out)   # ReLU 활성화 함수 적용\n",
    "        out = self.fc2(out)    # 두 번째 층에 통과\n",
    "        return out  \n",
    "\n",
    "# 포워드 패스\n",
    "#   - (10, 20)크기의 입력 x가 첫 번째 완전 연결층을 통과하여 (10, 50) 크기의 은닉층 출력 생성\n",
    "#   - ReLU 활성화 함수를 통과하고 크기는 동일하게 (10, 50) 유지\n",
    "#   - 두 번째 완전 연결층을 통과하여 (10, 3) 크기의 최종 출력 생성    \n",
    "# 최종 소프트맥스 층이 생략된 이유는 파이토치의 크로스 엔트로피 손실 함수가 내부적으로 소프트맥스 함수를 포함하고 있기 때문\n",
    "model = SimpleClassifier(input_dim, hidden_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4017b70e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [500/3000], Loss: 0.0032\n",
      "Step [1000/3000], Loss: 0.0007\n",
      "Step [1500/3000], Loss: 0.0003\n",
      "Step [2000/3000], Loss: 0.0002\n",
      "Step [2500/3000], Loss: 0.0001\n",
      "Step [3000/3000], Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # 크로스 엔트로피 손실 함수 정의\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저 정의\n",
    "\n",
    "for step in range(3000):\n",
    "    # 모델의 예측값 계산\n",
    "    outputs = model(vectors)\n",
    "    \n",
    "    # 손실 계산\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # 기울기 초기화\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 역전파 수행\n",
    "    loss.backward()\n",
    "    \n",
    "    # 가중치 업데이트\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (step+1) % 500 == 0:\n",
    "        print(f'Step [{step+1}/3000], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "132b4c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music\n",
      "science\n",
      "Document: \"Listening to rock music is fun.\" => Predicted class: music\n",
      "Document: \"I love science very much.\" => Predicted class: science\n"
     ]
    }
   ],
   "source": [
    "new_docs = [\n",
    "    \"Listening to rock music is fun.\",\n",
    "    \"I love science very much.\"\n",
    "]\n",
    "\n",
    "class_names = ['cinema', 'music', 'science']\n",
    "\n",
    "# 새로운 문서를 특성 벡터로 변환\n",
    "new_vectors = torch.tensor([doc_to_bow(doc, vocabulary) for doc in new_docs], dtype=torch.float)\n",
    "\n",
    "# 새로운 문서에 대한 예측 수행\n",
    "#   - 기울기 계산이 필요 없으므로 torch.no_grad() 컨텍스트 매니저 사용\n",
    "#   - 훈련이 아니기 때문에 메모리 사용량과 계산 속도 최적화\n",
    "with torch.no_grad():\n",
    "    # 모델에 새로운 벡터 입력하여 예측값 계산\n",
    "    #   - 모든 입력을 동시에 처리한다.\n",
    "    #   - 이런 병렬 처리 방식은 벡터화된 연산을 활용하며, 입력을 하난씩 처리하는 것보다 훨씬 효율적이다.\n",
    "    outputs = model(new_vectors)\n",
    "    predicted_ids = torch.argmax(outputs, dim=1)+1  # 가장 높은 점수를 가진 클래스 인덱스 선택\n",
    "    print(class_names[predicted_ids[0].item()-1])\n",
    "    print(class_names[predicted_ids[1].item()-1])\n",
    "    \n",
    "    for i, now_doc  in enumerate(new_docs):\n",
    "        print(f'Document: \"{now_doc}\" => Predicted class: {class_names[predicted_ids[i].item()-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf0d62",
   "metadata": {},
   "source": [
    "21. BoW 방식은 간단하고 실용적이지만 한계점이 있다.\n",
    "    - 토큰의 순서나 맥락을 포착하지 못한다.\n",
    "        - 예를 들어, \"The cat sat on the mat.\"과 \"On the mat sat the cat.\"은 동일한 BoW 표현을 가지지만, 문장의 의미는 다를 수 있다.\n",
    "        - n-그램(n-gram)은 이런 문제를 해결하기 위한 한가지 방법이다.\n",
    "            - n-그램은 연속된 n개의 토큰 시퀀스를 캡처한다.\n",
    "            - 예를 들어, 2-그램(바이그램)은 \"The cat\", \"cat sat\", \"sat on\"과 같은 토큰 쌍을 포함한다.\n",
    "            - n-그램을 사용하면 토큰 간의 일부 순서 정보를 포착할 수 있지만, 여전히 문맥과 장기 의존성을 완전히 포착하지는 못한다.\n",
    "            - 사용하는데 어휘사전 크기가 기하급수적으로 증가하는 단점이 있다.\n",
    "    - 어휘사전에 없는 단어를 처리하지 못한다.\n",
    "        - 새로운 단어가 나타나면 어휘사전을 업데이트하고 모델을 재훈련해야 한다.\n",
    "    - 동의어와 유사어를 처리하지 못한다.\n",
    "        - 예를 들어, \"car\"와 \"automobile\"은 동일한 의미를 가지지만, BoW 표현에서는 별개의 토큰으로 취급된다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
