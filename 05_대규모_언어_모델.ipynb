{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a181842f",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/the-lm-book/blob/main/05_대규모_언어_모델.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490f1bb7",
   "metadata": {},
   "source": [
    "# 목차\n",
    "* [Chapter 5_1 규모가 클수록 좋은 이유](#chapter5_1)\n",
    "* [Chapter 5_1_1 대규모 파라미터 개수](#chapter5_1_1) \n",
    "* [Chapter 5_1_2 큰 문맥 크기](#chapter5_1_2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5d8712",
   "metadata": {},
   "source": [
    "## Chapter 5_1 규모가 클수록 좋은 이유 <a class=\"anchor\" id=\"chapter5_1\"></a>\n",
    "1. LLM은 파라미터가 많고, 문맥 윈도가 크로, 상당한 컴퓨팅 자원을 바탕으로 대규모 말뭉치에서 훈련된다.\n",
    "    - 이런 규모 덕분에 복잡한 언어 패턴을 학습하고 심지어 정보를 기억할 수 있다.\n",
    "\n",
    "2. 대화를 이어가고 복잡한 지시를 따를 수 있는 채팅 LM을 만드는 작업은 두 단계로 구성된다.\n",
    "\n",
    "2. 1단계\n",
    "    - 수조 개의 토큰으로 구성된 대규모 데이터셋에서 사전훈련 진행\n",
    "        - 모델은 무맥을 바탕으로 다음 토큰을 예측하는 방법을 학습나다.\n",
    "    - 다음과 같은 문맥이 있다고 가정한다.\n",
    "        <pre>\n",
    "        \"The CRISPR-Cas9 technique has revolutionized genetic engineering by enabling precise modification of DNA sequences. \n",
    "        The process uses a guide RNA to target specific locations in the genome, allowing scientists to add, remove, or alter genetic material with unprecedented accuracy. \n",
    "        This technology has vast potential applications, including treating genetic disorders, improving crop resilience, and advancing our understanding of gene functions.\"\n",
    "        </pre>\n",
    "    - 다음 토큰을 정화하게 예측하려면 모델은 다음과 같은 내용을 알아야한다.\n",
    "        - CRISPR-Cas9 기술이 무엇인지\n",
    "        - 유전자 공학에서의 역할\n",
    "        - 가이드 RNA의 기능\n",
    "        - 유전체에서 특정 위치를 타겟팅하는 방법\n",
    "        - 유전 물질을 추가, 제거 또는 변경하는 방법\n",
    "        - 이 기술의 잠재적 응용 분야\n",
    "    - 잘 훈련된 LLM은 \"CRISPR-Cas9\"과 같은 복잡한 개념을 이해하고, 관련 정보를 기억하며, 문맥에 맞는 다음 토큰을 생성할 수 있다.\n",
    "    - 모델이 표면적인 패턴에 의지하지 않고 유전자 편집 과정에 대한 깊은 이해를 바탄으로 문맥을 임베딩 벡터로 인코딩해야한다.\n",
    "    - GPT-3은 비교적 복잡한 패턴을 이어가는 능력을 보여주었고, GPT-3.5와 GPT-4는 더욱 향상된 성능을 보여주었다.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a11eb6",
   "metadata": {},
   "source": [
    "### Chapter 5_1_1 대규모 파라미터 개수 <a class=\"anchor\" id=\"chapter5_1_1\"></a>\n",
    "1. LLM의 가장 놀라운 특징 중 하나는 엄청난 파라미터 개수이다.\n",
    "    - 최신 LLM은 수정 억 개 또는 심지어 수조 개의 파라미터를 가질 수 있다.\n",
    "\n",
    "2. 트랜스포모 모델에서 파라미터의 개수는 임배딩 차원과 디코더 블록 개수에 의해 크개 좌우된다.\n",
    "    - 이런 값이 커질수록 셀프 어텐션과 MLP 층에서 사용하는 임베딩 차원의 제곱 비율로 파라미터 개수가 증가하고, 디코더 블록의 개수와 함께 선형적으로 증가한다.\n",
    "    - 예를 들어, 임베딩 차원이 2048이고 디코더 블록 개수가 24인 트랜스포머 모델은 약 2억 5천만 개의 파라미터를 가진다.\n",
    "    - 임베딩 차원이 12288이고 디코더 블록 개수가 96인 모델은 약 1천 70억 개의 파라미터를 가진다.\n",
    "\n",
    "3. 이 책에서 만든 모델과 여러 오픈 웨이트 LLM의 측징을 다음과 같은 이미지에 확인할 수 있다.\n",
    "    - 관례적으로 오픈 웨이트 모델의 이름에서 \"B\"는 10억 단위의 파라미터 개수를의미한다.\n",
    "    - 70B 모델의 각 파라미터를 32비트 부동소수점으로 저장하려면 약 280GB의 메모리가 필요하다.\n",
    "    \n",
    "        - ![llm-parameters](image/04-00-llm-parameters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03e1bd8",
   "metadata": {},
   "source": [
    "### Chapter 5_1_2 큰 문맥 크기 <a class=\"anchor\" id=\"chapter5_1_2\"></a>\n",
    "1. 최신 LLM은 수천 개의 토큰, 심지어 수백만 개의 토큰까지도 문맥으로 처리할 수 있다.\n",
    "    - 예를 들어, GPT-4는 최대 128k 토큰의 문맥 크기를 지원한다.\n",
    "\n",
    "2. 트랜스포머 코델에서 긴 텍스트를 처리 때 가장 큰 어려움은 셀프 어텐션 메커니즘의 복잗도 계산에 있다.\n",
    "    - 길이가 n인 시퀸스의 경우 셀프 어텐션은 모든 토큰 쌍 사이의 어텐션 점수를 계산해야 하므로 시간 및 공간 복잡도는 2차 다항 복잡도에 해당하는 O(n^2)이다.\n",
    "    - 예를 들어, 문맥 크기가 1,024 토큰인 경우 셀프 어텐션 매트릭스는 1,024 x 1,024 = 1,048,576개의 어텐션 점수를 계산해야 한다.\n",
    "\n",
    "3.  구조를 개선하고 어텐션 계산을 최적화하여 문캑 크기를 늘릴 수 있다.\n",
    "    - 그룹 쿼리 어테션(GQA)과 같은 기법은 어텐션 계산을 단순화하여 긴 시퀸스를 더 효율적으로 처리할 수 있다.\n",
    "       - GQA는 쿼리 벡터를 그룹으로 묶어 각 그룹에 대해 단일 어텐션 점수를 계산한다.\n",
    "    - 플래시 어텐션은 메모리 사용량을 줄이고 어텐션 계산 속도를 높이는 효율적인 알고리즘이다.\n",
    "       - 플래시 어텐션은 어텐션 매트릭스를 한 번에 모두 계산하지 않고, 작은 청크로 나누어 처리하여 메모리 사용량을 줄인다.\n",
    "\n",
    "4. 일반적으로 LLM은 4K ~ 8K개의 토큰 정도의 짧은 문맥에서 사전훈련된다.\n",
    "    - 어텐션 메커니즘의 2차 다항도 복잡도로 인해 긴 시퀸스에서 훈련할 때 계산 비용이 급격히 증가하기 때문이다.\n",
    "\n",
    "5. 긴 문맥을 다루는 능력은 초기 훈련 후에 이어진 특별 단계인 긴 문맥 사전훈련을 통해 나타나며 훈련 순서는 다음과 같다.\n",
    "    - 긴 문맥을 위한 점진적 훈련\n",
    "        - 모델의 문맥 윈도를 여러 단계를 거쳐 4,000 ~ 8,000개의 토큰에서 128,000 ~ 256,000개의 토큰으로 점진적으로 늘린다.\n",
    "        - 문캑 길이를 증가시키고 모델이 두 가지 핵심 조건을 만족할 때까지 훈련한다.\n",
    "            - 짧은 문맥에서 달성한 성능을 회복하고 유지할 수 있어야 한다.\n",
    "            - \"모래사장에서 바늘 찾기\"평과와 같은 긴 문맥과 관련된 작업에서 성능이 향상되어야 한다.\n",
    "                - \"모래사장에서 바늘 찾기\"평가는 긴 문맥에서 중요한 정보를 찾아내는 모델의 능력을 평가하는 벤치마크이다.\n",
    "    - 셀프 어텐션을 위한 효율적인 규모 확장\n",
    "        - 2차 계산 복잡도를 다루기 위해 문맥 병렬화(context parallelism)와 같은 기법을 사용하여 긴 시퀸스를 여러 GPU에 분산시켜 처리한다.\n",
    "            - 문맥 병렬화는 긴 시퀸스를 여러 부분으로 나누어 각 부분을 별도의 GPU에서 처리하는 방법이다.\n",
    "            - all-gather 연산을 사용하여 각 GPU에서 계산된 어텐션 점수를 결합한다."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
