{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabb597a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/the-lm-book/blob/main/04_트랜스포머.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d540148",
   "metadata": {},
   "source": [
    "# 목차\n",
    "* [Chapter 3_0 개요](#chapter3_0)\n",
    "* [Chapter 3_1 디코더 블록](#chapter3_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733c424",
   "metadata": {},
   "source": [
    "## Chapter 3_0 개요 <a class=\"anchor\" id=\"chapter3_0\"></a>\n",
    "1. 트랜스포머 모델은 NLP 분야를 크개 발전시켰다.\n",
    "    - 넓은 범위의 의존성을 관리하는 데 있어서 RNN의 한계를 극복하고 입력 시퀸스를 병렬로 처리할 수 있다.\n",
    "\n",
    "2. 트랜스포머 모델에는 세 가지 주요 유형이 있다.\n",
    "    - 인코더-디코더 트랜스포머: 번역과 같은 시퀸스-투-시퀸스 작업에 사용된다.\n",
    "    - 인코더 전용 트랜스포머: BERT와 같은 모델로, 주로 이해 작업에 사용된다.\n",
    "    - 디코더 전용 트랜스포머: GPT 시리즈와 같은 모델로, 주로 생성 작업에 사용된다.\n",
    "\n",
    "3. 트랜스포머 구조의 핵심 구성 요소는 다음과 같다.\n",
    "    - 셀프 어텐션 메커니즘: 입력 시퀸스의 다른 위치에 있는 단어들과 얼마나 관련이 있는지 평가한다.\n",
    "    - 포지셔널 인코딩: 시퀸스 내에서 단어의 순서를 나타낸다.\n",
    "    - 피드포워드 신경망: 각 위치에서 독립적으로 적용되는 완전 연결 신경망이다.\n",
    "\n",
    "4. 트랜스포머는 모든 토큰을 동시에 처리한다.\n",
    "    - 토큰을 병렬 처리함에도 불구하고, 위치 인코딩을 사용해 순차적인 맥락을 유지한다.\n",
    "\n",
    "5. 디코더 기반 트랜스포머는 아래의 그림처럼 디코더 블록이라는 층을 수직으로 여러 개 쌓아 구성된다.\n",
    "    - 디코더를 훈련시키려면 입력 시퀸스와 한 토큰씩 앞으로 이동시킨 출력 시퀸스가 필요하다.\n",
    "\n",
    "        ![Transformer](image/04-00-transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359481f",
   "metadata": {},
   "source": [
    "## Chapter 3_1 디코더 블록 <a class=\"anchor\" id=\"chapter3_1\"></a>\n",
    "1. 디코더 블록은 두 개의 하위 층이 있다.\n",
    "    - 셀프 어텐션과 위치별 다층 퍼셉트론(MLP)이다.\n",
    "\n",
    "2. 디코더 블록에서 아래와 같은 작업이 이루어진다.\n",
    "    - 토큰 임베딩을 처리한다.\n",
    "    - 셀프 어텐션 층은 1부터 L까지 모든 토크 t에 대해 입력 임베딩 벡터 x_t를 세로운 벡터 g_t로 변환한다.\n",
    "       - L: 입력 길이\n",
    "    - 셀프 어텟션 다음에는 위치별 MLP가 각 벡터 g_t를 독립적으로 처리한다.\n",
    "    - 디코더 블록마다 독립적인 파라미터를 가진 MLP가 있다.\n",
    "    - MLP 하나가 g_t를 입력으로 받아 z_t를 출력한다.\n",
    "    - MLP가 출력하는 z_t의 개수는 입력 토큰 x_t의 개수와 같다.\n",
    "    - 다음 출력 벡터 z_t는 다음 디코더 블록의 입력으로 사용된다.\n",
    "    - 위의 과정이 모든 디코더 블록에서 반복되며, 출력 벡터의 개수는 입력 토큰 x_t의 개수와 같다.\n",
    "    \n",
    "        ![Decoder Block](image/04-01-decoder-block.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
