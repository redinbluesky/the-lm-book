{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dabb597a",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/redinbluesky/the-lm-book/blob/main/04_트랜스포머.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d540148",
   "metadata": {},
   "source": [
    "# 목차\n",
    "* [Chapter 4_0 개요](#chapter4_0)\n",
    "* [Chapter 4_1 디코더 블록](#chapter4_1)\n",
    "* [Chapter 4_2 셀프 어텐션](#chapter4_2)\n",
    "* [Chapter 4_3 위치별 다층 퍼셉트론](#chapter4_3)\n",
    "* [Chapter 4_4 로터리 위치 임베딩](#chapter4_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733c424",
   "metadata": {},
   "source": [
    "## Chapter 4_0 개요 <a class=\"anchor\" id=\"chapter4_0\"></a>\n",
    "1. 트랜스포머 모델은 NLP 분야를 크개 발전시켰다.\n",
    "    - 넓은 범위의 의존성을 관리하는 데 있어서 RNN의 한계를 극복하고 입력 시퀸스를 병렬로 처리할 수 있다.\n",
    "\n",
    "2. 트랜스포머 모델에는 세 가지 주요 유형이 있다.\n",
    "    - 인코더-디코더 트랜스포머: 번역과 같은 시퀸스-투-시퀸스 작업에 사용된다.\n",
    "    - 인코더 전용 트랜스포머: BERT와 같은 모델로, 주로 이해 작업에 사용된다.\n",
    "    - 디코더 전용 트랜스포머: GPT 시리즈와 같은 모델로, 주로 생성 작업에 사용된다.\n",
    "\n",
    "3. 트랜스포머 구조의 핵심 구성 요소는 다음과 같다.\n",
    "    - 셀프 어텐션 메커니즘: 입력 시퀸스의 다른 위치에 있는 단어들과 얼마나 관련이 있는지 평가한다.\n",
    "    - 포지셔널 인코딩: 시퀸스 내에서 단어의 순서를 나타낸다.\n",
    "    - 피드포워드 신경망: 각 위치에서 독립적으로 적용되는 완전 연결 신경망이다.\n",
    "\n",
    "4. 트랜스포머는 모든 토큰을 동시에 처리한다.\n",
    "    - 토큰을 병렬 처리함에도 불구하고, 위치 인코딩을 사용해 순차적인 맥락을 유지한다.\n",
    "\n",
    "5. 디코더 기반 트랜스포머는 아래의 그림처럼 디코더 블록이라는 층을 수직으로 여러 개 쌓아 구성된다.\n",
    "    - 디코더를 훈련시키려면 입력 시퀸스와 한 토큰씩 앞으로 이동시킨 출력 시퀸스가 필요하다.\n",
    "\n",
    "        ![Transformer](image/04-00-transformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7359481f",
   "metadata": {},
   "source": [
    "## Chapter 4_1 디코더 블록 <a class=\"anchor\" id=\"chapter4_1\"></a>\n",
    "1. 디코더 블록은 두 개의 하위 층이 있다.\n",
    "    - 셀프 어텐션과 위치별 다층 퍼셉트론(MLP)이다.\n",
    "\n",
    "2. 디코더 블록에서 아래와 같은 작업이 이루어진다.\n",
    "    - 토큰 임베딩을 처리한다.\n",
    "    - 셀프 어텐션 층은 1부터 L까지 모든 토크 t에 대해 입력 임베딩 벡터 x_t를 세로운 벡터 g_t로 변환한다.\n",
    "       - L: 입력 길이\n",
    "    - 셀프 어텟션 다음에는 위치별 MLP가 각 벡터 g_t를 독립적으로 처리한다.\n",
    "    - 디코더 블록마다 독립적인 파라미터를 가진 MLP가 있다.\n",
    "    - MLP 하나가 g_t를 입력으로 받아 z_t를 출력한다.\n",
    "    - MLP가 출력하는 z_t의 개수는 입력 토큰 x_t의 개수와 같다.\n",
    "    - 다음 출력 벡터 z_t는 다음 디코더 블록의 입력으로 사용된다.\n",
    "    - 위의 과정이 모든 디코더 블록에서 반복되며, 출력 벡터의 개수는 입력 토큰 x_t의 개수와 같다.\n",
    "    \n",
    "        ![Decoder Block](image/04-01-decoder-block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4e5ea1",
   "metadata": {},
   "source": [
    "## Chapter 4_2 셀프 어텐션 <a class=\"anchor\" id=\"chapter4_2\"></a>\n",
    "1. 5개의 토큰을 가진 샘플 [\"We\",\"train\",\"a\",\"transformer\",\"model\"]과 최대 입력 길이가 4인 디코더가 있다고 가정\n",
    "    - 각 토큰은 6차원 임베딩 벡터로 표현된다고 가정\n",
    "    - 각 디코더 블록에서 셀프 어텐션은 훈련 가능한 세 개의 파라미터 행렬 W<sup>q</sup>, W<sup>k</sup>, W<sup>v</sup>를 사용\n",
    "        - q: 쿼리(query), k: 키(key), v: 값(value)\n",
    "        - 파라미터 행렬의 텐서의 크기가 6 * 6 이라고 가정\n",
    "    - 입력 임베딩 벡터(4 * 6)와 훈련 가능한 파라미터(6 * 6)와 곱해져 쿼리 행렬 Q, 키 행렬 K, 값 행렬 V(각각 4*6)가 생성된다.\n",
    "\n",
    "2. \"train\" 단어를 표현하는 두 번째 토큰 x<sub>2</sub>에 대한 출력 g<sub>2</sub>을 계산하기 위해 셀프 어텐션 층은 6단계를 거친다.\n",
    "\n",
    "3. 첫 번째 단계\n",
    "    - 아래와 그림과 같이 쿼리 행렬 Q, 키 행렬 K, 값 행렬 V를 계산한다.\n",
    "    - 임베딩 벡터 x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, x<sub>4</sub>에 각각 W<sup>q</sup>, W<sup>k</sup>, W<sup>v</sup>를 곱한다.\n",
    "    - 행렬 곱셈의 결과로 쿼리 행렬 Q, 키 행렬 K, 값 행렬 V가 생성된다.\n",
    "    - 이 행렬은 각각 4행 6열의 크기를 가진다.\n",
    "    - 각각의 입력 임베딩 K<sub>t</sub>는 쿼리 벡터 Q<sub>t</sub>와 값 벡터 V<sub>t</sub>, 키 벡터 K<sub>t</sub>와 매핑된다.\n",
    "    \n",
    "        ![Self-Attention](image/04-02-self-attention-1.png)\n",
    "\n",
    "4. 두 번째 단계\n",
    "    - 두 번째 토큰 \"train\"을 처리하는 것을 가정한다.\n",
    "    - 쿼리 행렬 Q에서 두 번째 행 Q<sub>2</sub>와 키 행렬을 점곱하여 어텐션 점수를 계산한다.\n",
    "        - Q<sub>2</sub> • K<sub>1</sub> = 4.90\n",
    "        - Q<sub>2</sub> • K<sub>2</sub> = 17.15\n",
    "        - Q<sub>2</sub> • K<sub>3</sub> = 9.80\n",
    "        - Q<sub>2</sub> • K<sub>4</sub> = 12.25\n",
    "    - 벡터로 나타내면 아래와 같다.\n",
    "        - 어텐션 점수<sub>2</sub> = [4.90, 17.15, 9.80, 12.25]<sup>T</sup>\n",
    "    \n",
    "5. 세 번째 단계\n",
    "    - 어텐션 점수를 스케일링한다.\n",
    "    - 어텐션 점수를 키 벡터의 차원 수의 제곱근(√6 ≈ 2.45)으로 나눈다.\n",
    "    - 스케일된 어텐션 점수<sub>2</sub> =[4.90/2.45, 17.15/2.45, 9.80/2.45, 12.25/2.45]<sup>T</sup>=[2.00, 7.00, 4.00, 5.00]<sup>T</sup>\n",
    "\n",
    "6. 네 번째 단계\n",
    "    - 스케일된 어텐션 점수에 코잘 마스크를 적용한다.\n",
    "        - 코잘 마스크는 미래 토큰에 대한 어텐션 점수를 음의 무한대로 설정한다.\n",
    "    - 두 번째 입력 위치에 대한 코잘 마스크는 아래와 같다.\n",
    "        - 코잘 마스크<sub>2</sub> =[0, 0, -∞, -∞]<sup>T</sup>\n",
    "    - 스케일 조정된 점수에 코잘 마스크를 더해 마스크드 점수를 얻는다.\n",
    "        - 마스크드 점수<sub>2</sub> = 스케일 조정됨 점수<sub>2</sub> + 코잘 마스크<sub>2</sub> = [2.00, 7.00, -∞, -∞]<sup>T</sup>\n",
    "\n",
    "7. 다섯 번째 단계\n",
    "    - 마스크드 점수에 소프트맥스 함수를 적용해 어텐션 가중치를 계산한다.\n",
    "    - 어텐션 가중치<sub>2</sub> = softmax([2.00, 7.00, -∞, -∞]) \n",
    "    - = [e^(2.00)/ (e^(2.00) + e^(7.00)), e^(7.00)/ (e^(2.00) + e^(7.00)), 0.0, 0.0]<sup>T</sup> \n",
    "    - = [0.0025, 0.9975, 0.0, 0.0]<sup>T</sup>\n",
    "    - 어덴션 점수를 키 차원의 제곱근으로 나누면 차원이 커짐에 따라 점곱 결과가 너무 커지는 것을 방지할 수 있다.\n",
    "    - 큰 점곱과 소프트맥스를 적용하면 그레이디언트가 매우 작아져 학습이 어려워진다.\\\n",
    "    \n",
    "8. 여섯 번째 단계\n",
    "    - 어텐션 가중치와 값 벡터 V<sub>1</sub>, V<sub>2</sub>, V<sub>3</sub>, V<sub>4</sub>를 가중합해 출력 벡터 g<sub>2</sub>를 계산한다.\n",
    "        - g<sub>2</sub> ≈ 0.0025 * V<sub>1</sub> + 0.9975 * V<sub>2</sub> + 0.0 * V<sub>3</sub> + 0.0 * V<sub>4</sub>\n",
    "    - 위치 2에 대한 디코더의 출력은 위치 1과 위치 2에 있는 입력에만 의존며, 2의 출력에 좀더 큰 가중치를 부여한다.\n",
    "    - 미래 위치를 참조하지 못하도록 하는 코잘 마스크의 효과이다.\n",
    "    - 이 예제는 두 번째 토큰이 자기 자신에게 주의를 기울이지만, 문맥에 따라 다른 토큰에 주의를 기울일 수도있다.\n",
    "\n",
    "9. 벡터 Q, K, V는 다음과 같이 이해할 수 있다.\n",
    "    - 쿼리 벡터 Q<sub>t</sub>\n",
    "        - 토큰 임베딩은 다른 위치에 대한 정보를 찾는다.\n",
    "        - 예를 들어 \"I\"라는 단어는 \"am\"이라는 단어를 찾을 수 있다.\n",
    "        - 모든 위치 t를 위한 쿼리 벡터 Q<sub>t</sub>는 해당 위치에서 찾고자 하는 정보를 나타낸다.\n",
    "    - 키 벡터 K<sub>t</sub>\n",
    "        - Q<sub>t</sub>와 각 위치에 있는 K<sub>t</sub>를 점곱해 두 벡터 사이의 유사도를 계산한다.\n",
    "        - 점곱 결과가 클수록 두 벡터가 유사하다는 의미이다.\n",
    "    - 값 벡터 V<sub>t</sub>\n",
    "        - 어떤 위치 t의 K<sub>t</sub>가 쿼리 벡터 Q<sub>i</sub>와 유사하다면, 해당 위치 t의 값 벡터 V<sub>t</sub>가 출력에 더 큰 영향을 미친다.\n",
    "\n",
    "10. 어텐션 개념은 트랜스포머가 나오기 전에 등장했다.\n",
    "    - 2014년 요수아 벤지오의 지도하에서 연구하던 드미트리 바흐나나흐는 번역 작업에서 어텐션 메커니즘을 사용한 최초의 논문을 발표했다.\n",
    "    - 바흐다나우는 영어를 배울 때 문장의 여러 다른 부분에 초점을 맞췄던 자신의 학습 경험을 토대로 RNN이 각 번역 단계에서 가장 주요한 입력 단어에 집중할 수 있도록 어텐션 메커니즘을 고안했다.\n",
    "\n",
    "11. g<sub>2</sub> 계산에 사용된 과정이 입력 시퀸스에 있는 다른 위치에도 반복되어 출력 벡터 g<sub>1</sub>, g<sub>3</sub>, g<sub>4</sub>가 생성된다.\n",
    "    - 이 경우 코잘 마스크는 아래와 같이 적용된다.\n",
    "        - g<sub>1</sub>: 코잘 마스크<sub>1</sub> =[0, -∞, -∞, -∞]<sup>T</sup>\n",
    "        - g<sub>3</sub>: 코잘 마스크<sub>3</sub> =[0, 0, 0, -∞]<sup>T</sup>\n",
    "        - g<sub>4</sub>: 코잘 마스크<sub>4</sub> =[0, 0, 0, 0]<sup>T</sup>\n",
    "    - 첫 번째 토큰은 자기 자신에게만 주의를 기울이고, 두 번째 토큰은 첫 번째와 두 번째 토큰에만 주의를 기울인다.\n",
    "\n",
    "12. 모든 위치에 대한 어텐션을 계산하는 일반화된 공식은 아래와 같다.\n",
    "    - 어텐션(Q, K, V) = softmax( (Q • K<sup>T</sup>) / √d<sub>k</sub> + M ) • V\n",
    "        - d<sub>k</sub>: 키 벡터의 차원 수\n",
    "        - M: 코잘 마스크 행렬\n",
    "        - Q, V: (L, d<sub>model</sub>) 크기의 행렬\n",
    "        - K: (L, d<sub>k</sub>) 크기의 행렬\n",
    "        - L: 입력 길이\n",
    "    - QK<sup>T</sup>는 한번에 모든 어텐션 점수를 계산하므로 훨씬 빠르다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35fb40be",
   "metadata": {},
   "source": [
    "## Chapter 4_3 위치별 다층 퍼셉트론 <a class=\"anchor\" id=\"chapter4_3\"></a>\n",
    "1. 마스크드 셀프 어텐션 층 다음에 출력 벡터 g<sub>t</sub>가 다층 퍼셉트론(MLP)에서 독립적으로 처리된다.\n",
    "    - 일부 자료에서는 피드포워드 신경망, 완전 연결층 이라고 부른다.\n",
    "    - 아래와 같 식의 변환을 적용한다.\n",
    "        - z<sub>t</sub> = W<sub>2</sub>(Relu(W<sub>1</sub>g<sub>t</sub> + b<sub>1</sub>) + b<sub>2</sub>)\n",
    "            - W<sub>1</sub>, W<sub>2</sub>: 훈련 가능한 가중치 행렬\n",
    "            - b<sub>1</sub>, b<sub>2</sub>: 훈련 가능한 바이어스 벡터\n",
    "    - Z<sub>t</sub>는 다음 디코더 블록의 입력으로 사용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf59a5b9",
   "metadata": {},
   "source": [
    "## Chapter 4_4 로터리 위치 임베딩 <a class=\"anchor\" id=\"chapter4_4\"></a>\n",
    "1. 트랜스포머는 순차적인 의존성 없이 모든 토큰에 대해 한 번에 어텐션 점수를 계산한다.\n",
    "    - 그러나 자연어는 순차적인 특성을 가지므로, 토큰의 순서를 모델에 제공해야 한다.\n",
    "    - 이를 위해 트랜스포머는 위치 임베딩을 사용한다.\n",
    "\n",
    "2. 단어의 순서를 고려하려면 트랜스포머는 위치 정보를 통합해야 한다.\n",
    "\n",
    "3. 널리 사용되는 방법은 로터리 위치 임베딩(RoPE, Rotary Position Embedding)이다.\n",
    "    - 어텐션 메커니즘 안에서 쿼리 벡터와 키 벡터를 위치에 따라 회전시킨다.\n",
    "    - 훈련 과정에서 본 것보다 더 킨 시퀸스를 효과적으로 일반화 할 수 있다.\n",
    "    - 시간과 자원을 절약하기 위해 짧은 시퀸스에서 훈련 후 추론 할 때 긴 시퀸스를 처리할 수 있다.\n",
    "\n",
    "4. RoPE는 쿼리 벡터와 키 키 벡터를 회전하여 위치 정보를 인코딩한다.\n",
    "    - 어텐션을 계산하기 전에 이 회전이 수행된다.\n",
    "\n",
    "5. 이미지로 표현하면 아래와 같다.\n",
    "    - \"원본\"이라고 표시된 검은 화살표는 셀프 어텐션에 있는 위치 정보가 없는 키 벡터나 쿼리 벡터를 나타낸다.\n",
    "    - RoPE는 이 벡터를 토큰 위치에 따라 회전 시킴으로써 위치 정보를 임베딩한다.\n",
    "        - 쿼리와 키 벡터 전체를 회전시키는 것이 아니라 벡터 안에 있는 인접한 차원 쌍을 회전시킨다.\n",
    "    - 색깔이 있는 화살표는 위치 1, 3, 5, 7에 대해 회전된 결과 벡터를 나타낸다.\n",
    "    - RoPE의 핵심 특징은 회전한 두 벡터 사이의 각도가 시퀸스에 있는 두 벡터 사이의 거리를 인코딩한 것이다.\n",
    "        - 위치 1과 위치 3 사이의 각도는 위치 5와 위치 7 사이의 각도와 같다.\n",
    "        - 위치 사이의 거리가 동일하게 두 위치만큼 떨어져 있기 때문이다.\n",
    "\n",
    "        ![RoPE](image/04-03-rope.png)\n",
    "\n",
    "6. 회전을 위해 회전 행렬을 사용한다.\n",
    "    - 회전 행렬은 벡터의 각도와 크기를 유지하면서 벡터를 회전시킨다.\n",
    "    - RoPE는 쿼리 벡터와 키 벡터의 인접한 두 차원에 회전 행렬을 적용한다.\n",
    "    - 각 차원 쌍에 대해 서로 다른 각도를 사용하여 위치 정보를 인코딩한다.\n",
    "    - 2차원의 경우 각도 θ에 대한 회전 행렬 R(θ)는 아래와 같다.\n",
    "        - R(θ) = [[cosθ, -sinθ], [sinθ, cosθ]]\n",
    "\n",
    "7. 2차원 백터 q = [2, 1]<sup>T</sup>를 회전한다고 가정한다.\n",
    "    - 이 벡터를 회전시키려면  q와 회전 행렬 R(θ)를 곱한다.\n",
    "    - 만들어진 새로운 벡터는 각도 θ만큼 반시계 방향으로 회전한 q'를 나타낸다.\n",
    "    - 45도 회전(θ = π/4 라디안)의 경우 특별히 아래의 식을 사용할 수 있다\n",
    "        - cos(π/4) = sin(π/4) = √2/2 ≈ 0.7071\n",
    "    - 45도의 회전 행렬은 아래와 같다.\n",
    "        - R(π/4) = [[√2/2, -√2/2], [√2/2, √2/2]]\n",
    "    - 따라서 벡터 q를 45도 회전시키면 아래와 같다.\n",
    "    - q' = R(π/4) • q = [[√2/2, -√2/2], [√2/2, √2/2]] • [2, 1]<sup>T</sup>\n",
    "    - = [(2√2/2) - (1√2/2), (2√2/2) + (1√2/2)]<sup>T</sup> = [0.7071, 2.1213]<sup>T</sup>\n",
    "    - 그림의로 표현하면 아래와 같다.\n",
    "\n",
    "        ![Rotation Example](image/04-04-rotation-example.png)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pybuild",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
